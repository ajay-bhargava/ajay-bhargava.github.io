<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tracking the precise dropoff location of a delivery in your building. | Ajay Bhargava Ph.D. </title> <meta name="author" content="Ajay Bhargava Ph.D."> <meta name="description" content="Reflections on building a time-series classification model that uses the data from the 9-DoF sensor on your phone."> <meta name="keywords" content="ai-enginer, ml-engineer, full-stack-engineer, fractional-cto, bench-scientist"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%94&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ajay-bhargava.github.io/blog/2025/timeseries-machine-learning/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Ajay Bhargava Ph.D. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">resume </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tracking the precise dropoff location of a delivery in your building.</h1> <p class="post-meta"> Created on June 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/timeseries"> <i class="fa-solid fa-hashtag fa-sm"></i> timeseries</a>   <a href="/blog/tag/9-dof"> <i class="fa-solid fa-hashtag fa-sm"></i> 9-DoF</a>   <a href="/blog/tag/sensor"> <i class="fa-solid fa-hashtag fa-sm"></i> sensor</a>   <a href="/blog/tag/modal"> <i class="fa-solid fa-hashtag fa-sm"></i> modal</a>   <a href="/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> python</a>   <a href="/blog/tag/tsai"> <i class="fa-solid fa-hashtag fa-sm"></i> tsai</a>   ·   <a href="/blog/category/machine-learning-posts"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning-posts</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"><a href="#tldr">Tl;DR</a></li> <li class="toc-entry toc-h1"> <a href="#things-you-can-do-with-time-series-data">Things you can do with time-series data</a> <ul> <li class="toc-entry toc-h2"><a href="#learn-how-to-sleep-better">Learn how to sleep better</a></li> <li class="toc-entry toc-h2"><a href="#have-a-job-that-cant-be-stolen-by-ai">Have a job that can’t be stolen by AI</a></li> <li class="toc-entry toc-h2"><a href="#learn-why-cancer-grows-the-way-it-does">Learn why cancer grows the way it does</a></li> <li class="toc-entry toc-h2"> <a href="#track-exactly-where-your-package-was-dropped-off">Track exactly where your package was dropped off</a> <ul> <li class="toc-entry toc-h3"><a href="#doorstepais-sdk-and-data-collection">Doorstep.ai’s SDK and data collection</a></li> <li class="toc-entry toc-h3"><a href="#9-dof-sensor-data">9-DoF sensor data</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"><a href="#finding-phone-sensor-data">Finding phone sensor data</a></li> <li class="toc-entry toc-h1"> <a href="#preparing-the-data-for-a-model">Preparing the data for a model</a> <ul> <li class="toc-entry toc-h2"><a href="#downloading-the-dataset">Downloading the dataset</a></li> <li class="toc-entry toc-h2"> <a href="#discovering-temporal-windows">Discovering temporal windows</a> <ul> <li class="toc-entry toc-h3"><a href="#intuition">Intuition</a></li> <li class="toc-entry toc-h3"><a href="#implementation">Implementation</a></li> <li class="toc-entry toc-h3"><a href="#visual-description">Visual Description</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#cleaning-and-preparing-the-dataset">Cleaning and preparing the dataset</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="tldr">Tl;DR</h1> <p>I built a time-series classification model that could classify whether or not a person was indoors or outdoors based on the data collected by the 9-DoF sensor on their phone. The github repository is <a href="https://github.com/ajay-bhargava/extrasensory" rel="external nofollow noopener" target="_blank">here</a>.</p> <h1 id="things-you-can-do-with-time-series-data">Things you can do with time-series data</h1> <p>It is no secret that I love time-series datastreams and I think humanity should get really good at reading, understanding, leveraging, and making predictions from time-series data.</p> <p>I’ve gleaned some insights while using time-series datasets over the years. Here’s a few:</p> <h2 id="learn-how-to-sleep-better">Learn how to sleep better</h2> <p>When I worked on a project with <a href="https://www.fulcradynamics.com/magazine/are-your-devices-trying-to-tell-you-something-unlocking-personal-health-insights-through-data-with-data-scientist-ajay-bhargava" rel="external nofollow noopener" target="_blank">Fulcra Dynamics</a> I leveraged HealthKit data and other raw data from a continuous glucose monitor. I assembled data at a per second interval and cleansed over 6 months of collection data to produce a multi-modal machine learning approach to find that a patient could get one extra hour of sleep if they just ate their last meal 2-3 hours earlier than they normally did. Intriguingly this is very consistent with the work of <a href="https://en.wikipedia.org/wiki/J%C3%BCrgen_Aschoff" rel="external nofollow noopener" target="_blank">Jürgen Aschoff</a> who found that the body’s circadian rhythm is regulated by multiple cues, including the timing of meals.</p> <p>This dataset was incredibly rich and clean from biased actions because the data was passively collected and required no human intervention. Sure, you could knock the collection accuracy and the precision of the data relative to the human event but that’s an improving trend over time as electronics become more sensitive and the data collection becomes more precise.</p> <h2 id="have-a-job-that-cant-be-stolen-by-ai">Have a job that can’t be stolen by AI</h2> <p>Its no secret that time-series models are incredibly dense matricies. In a typical time-series tensor you’ve got three dimensions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">],</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
    <span class="p">]</span>
<span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># &gt; (2, 2, 3)
</span>
</code></pre></div></div> <p>Where the first dimension is the sample you’re looking at and the second and third dimension could be the number of time-stamps collected and the number of features collected at each time-stamp. Consider that for each feature you may have a <code class="language-plaintext highlighter-rouge">np.float</code> at each dimensional position. If you were to place that entire tensor into the context window of a model, you’d very easily overwhelm the model’s window to accept your request. In fact, this is exactly why Google and others developed vision langugage models (vLLM’s) to handle dealing with time-series data in a way that’s more information efficient. I discuss this here:</p> <div class="jekyll-twitter-plugin"> <blockquote class="twitter-tweet" align="center"> <p lang="en" dir="ltr">Remember kids: time-series data analysis with GPT's isn't gonna happen with long context windows eating raw time-series, its gonna happen with vLLM's understanding matplotlib plots: <a href="https://t.co/3F5CxVLZ7y" rel="external nofollow noopener" target="_blank">https://t.co/3F5CxVLZ7y</a></p>— AJ (@0x1F9ED) <a href="https://twitter.com/0x1F9ED/status/1886227564173111657?ref_src=twsrc%5Etfw" rel="external nofollow noopener" target="_blank">February 3, 2025</a> </blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h2 id="learn-why-cancer-grows-the-way-it-does">Learn why cancer grows the way it does</h2> <p>Lots of people today are pilled on either single-cell genomics or spatial transcriptomics. While these are either destructive or non-destructive ways to study cancer cells or cancer tissue <em>in-situ</em> they are not very powerful tools because cancer grows over space, and, crucially, <strong>time</strong>. If people do study cancer cells over time, they’re usually piecing a timeline of events from multiple populations of cancers across different patients and its usually met with mixed results each time.</p> <p>The holy grail is to study the same cancer over time from the initial cell all the way to the final size it grows against (whether that’s when the patient is dead or when the cancer hits a boundary condition). Fortunately, I devised a way to study this (forthcoming blog post). What I can say is that when I did study it this way, I found out <strong>why</strong> cancer tissue grows in a similar pattern every time, even though no two cells are identical.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clonal-dynamics-480.webp 480w,/assets/img/clonal-dynamics-800.webp 800w,/assets/img/clonal-dynamics-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/clonal-dynamics.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>To create time-series data from imaging data is incredibly challenging and yet incredibly powerful because you can glean how each individual cell behaves within a crowded space over time. By leveraging the right analysis methods for spatiotemporal datasets, you can essentially study matter in the way it actually exists in our world.</p> <h2 id="track-exactly-where-your-package-was-dropped-off">Track <em>exactly</em> where your package was dropped off</h2> <p>Ok, now on to the subject of this post. I’ve been fascinated by the value proposition of <a href="http://www.doorstep.ai" rel="external nofollow noopener" target="_blank">Doorstep.ai</a>. It appears from their website that they are a company that is interested in tracking the last 50ft of the delivery into your building. If you live in a major city and you don’t have a doorman in your building, you know exactly what this pain is.</p> <p>According to a <a href="https://chatgpt.com/share/685072fe-a21c-8012-97e4-c71c1e82b4bf" rel="external nofollow noopener" target="_blank">ChatGPT</a> driven business estimation via web-search, U.S. carriers absorb roughly $16 billion in write-offs every year when parcels disappear inside buildings or get mis-scanned at the door. A 9-DoF sensor fusion SDK that runs on a driver’s phone to generate a verifiable indoor drop-point coordinate could cut “lost in the last 50 ft” claims by about 30 percent and turning those savings into measurable ROI.</p> <h3 id="doorstepais-sdk-and-data-collection">Doorstep.ai’s SDK and data collection</h3> <p>Intriguingly they have a <a href="https://www.doorstep.ai/docs" rel="external nofollow noopener" target="_blank">SDK</a> that ostensibly runs on the delivery driver’s phone.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nx">React</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">react</span><span class="dl">'</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">DoorstepAIRoot</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">@doorstepai/dropoff-sdk</span><span class="dl">"</span><span class="p">;</span>
<span class="kd">const</span> <span class="nx">App</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">(</span>
  <span class="o">&lt;</span><span class="nx">DoorstepAIRoot</span> <span class="nx">apiKey</span><span class="o">=</span><span class="dl">"</span><span class="s2">YOUR_API_KEY</span><span class="dl">"</span><span class="o">&gt;</span>
<span class="p">);</span>
<span class="k">export</span> <span class="k">default</span> <span class="nx">App</span><span class="p">;</span>

<span class="c1">// Injection of collection data via SDK</span>
<span class="c1">// Start with Google PlaceID</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByPlaceID</span><span class="p">(</span><span class="dl">"</span><span class="s2">placeID_here</span><span class="dl">"</span><span class="p">);</span>

<span class="c1">// Start with Google PlusCode</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByPlusCode</span><span class="p">(</span><span class="dl">"</span><span class="s2">plusCode_here</span><span class="dl">"</span><span class="p">);</span>

<span class="c1">// Start with Address Components</span>
<span class="kd">const</span> <span class="nx">address</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">street_number</span><span class="p">:</span> <span class="dl">"</span><span class="s2">123</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">route</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Main St</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">locality</span><span class="p">:</span> <span class="dl">"</span><span class="s2">City</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">administrative_area_level_1</span><span class="p">:</span> <span class="dl">"</span><span class="s2">State</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">postal_code</span><span class="p">:</span> <span class="dl">"</span><span class="s2">12345</span><span class="dl">"</span>
<span class="p">};</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByAddressType</span><span class="p">(</span><span class="nx">address</span><span class="p">);</span>
</code></pre></div></div> <h3 id="9-dof-sensor-data">9-DoF sensor data</h3> <p>The basic premise of a 9-DoF sensor is that it can collect data from the following sensors:</p> <table> <thead> <tr> <th style="text-align: left">Sensor</th> <th style="text-align: center">Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Accelerometer</td> <td style="text-align: center">3-axis accelerometer</td> </tr> <tr> <td style="text-align: left">Gyroscope</td> <td style="text-align: center">3-axis gyroscope</td> </tr> <tr> <td style="text-align: left">Magnetometer</td> <td style="text-align: center">3-axis magnetometer</td> </tr> <tr> <td style="text-align: left">Light</td> <td style="text-align: center">Light sensor</td> </tr> <tr> <td style="text-align: left">Audio</td> <td style="text-align: center">Audio sensor</td> </tr> <tr> <td style="text-align: left">Barometer</td> <td style="text-align: center">Barometric pressure</td> </tr> <tr> <td style="text-align: left">GPS</td> <td style="text-align: center">Global Positioning System</td> </tr> </tbody> </table> <hr> <p>These chips are on the phone’s System on a Chip (SoC) but conceptually they are measuring the movement of a “ball” inside a box with sensor endings. Graphically, it looks like this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-dof-sensor-480.webp 480w,/assets/img/9-dof-sensor-800.webp 800w,/assets/img/9-dof-sensor-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/9-dof-sensor.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The 9-DoF sensor is a very powerful sensor that can be used to track the movement of a “ball” inside a box with sensor endings.</p> <blockquote> <p>I sought out to figure out how to build a ML model that could carry out useful classifications on the data collected by this SDK.</p> </blockquote> <p>Now, because I don’t have access to <a href="http://www.doorstep.ai" rel="external nofollow noopener" target="_blank">Doorstep.ai</a>’s data, I had to find my own. Below and in the rest of the blog post, I yap about my casual 2-day journey to build a model and inference server that could run on the data collected by this SDK. Ostensibly by the end you will learn how I have deployed an inference server that could be passed the data collected by the SDK and be used to fill out a dropoff report that a customer could access if they have trouble locating the parcel in their building.</p> <h1 id="finding-phone-sensor-data">Finding phone sensor data</h1> <p>There are many datasets available. I ended up going for the <strong>ExtraSensory</strong> dataset because it did not require too much data cleaning and preparation to get started, though, the SHL dataset is <em>very</em> appealing.</p> <table> <thead> <tr> <th>Dataset (year)</th> <th>Core modalities present</th> <th>Needs a “GPU”?</th> <th>Inside/Outside labels?</th> <th>Commentary</th> </tr> </thead> <tbody> <tr> <td><strong>ExtraSensory (UC San Diego, 2017)</strong></td> <td>Acc, Gyr, Mag, Air pressure, <strong>Light</strong>, <strong>22 kHz audio</strong>, phone-state, GPS, temp/humidity</td> <td>60 users ⇒ 302 k 20 s windows (≈22 h raw audio + high-freq motion; ~8 GB raw, ~215 MB features) (<a href="https://dcase-repo.github.io/dcase_datalist/datasets/scenes/extrasensory.html" title="DCASE Datalist / ExtraSensory Dataset" rel="external nofollow noopener" target="_blank">dcase-repo.github.io</a>, <a href="https://www.kaggle.com/datasets/yvaizman/the-extrasensory-dataset?utm_source=chatgpt.com" title="The ExtraSensory Dataset - Kaggle" rel="external nofollow noopener" target="_blank">kaggle.com</a>, <a href="https://dcase-repo.github.io/dcase_datalist/datasets/scenes/extrasensory.html?utm_source=chatgpt.com" title="DCASE Datalist / ExtraSensory Dataset" rel="external nofollow noopener" target="_blank">dcase-repo.github.io</a>)</td> <td>Yes (self-reported “indoors/outdoors” &amp; “in vehicle”) (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10915014/?utm_source=chatgpt.com" title="Robust human locomotion and localization activity recognition over ..." rel="external nofollow noopener" target="_blank">pmc.ncbi.nlm.nih.gov</a>)</td> <td> <em>Full multimodal fusion</em> + noisy labels + class imbalance</td> </tr> <tr> <td><strong>Sussex-Huawei Locomotion (SHL, 2018)</strong></td> <td>Acc, Gyr, Mag, <strong>Microphone audio</strong>, Baro, GPS, humidity, 15 total phone sensors (<a href="https://www.researchgate.net/figure/Sensor-modalities-sampling-rate-in-the-complete-SHL-dataset_tbl1_354740102?utm_source=chatgpt.com" title="Sensor modalities (sampling rate) in the complete SHL dataset." rel="external nofollow noopener" target="_blank">researchgate.net</a>, <a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices" rel="external nofollow noopener" target="_blank">eecs.qmul.ac.uk</a>)</td> <td>2 812 h; 950 GB raw; 4 phones × 59 days preview (~20 GB) (<a href="https://www.shl-dataset.org/dataset/?utm_source=chatgpt.com" title="Sussex-Huawei Locomotion Dataset" rel="external nofollow noopener" target="_blank">shl-dataset.org</a>, <a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices" rel="external nofollow noopener" target="_blank">eecs.qmul.ac.uk</a>)</td> <td>Yes (explicit inside/outside tag) (<a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices" rel="external nofollow noopener" target="_blank">eecs.qmul.ac.uk</a>)</td> <td> <em>Scale &amp; placement robustness</em>; show distributed training</td> </tr> <tr> <td><strong>OutFin (Scientific Data, 2021)</strong></td> <td>Acc, Gyr, Mag, <strong>Baro</strong>, <strong>Light</strong> + Wi-Fi, BT, cellular (<a href="https://www.nature.com/articles/s41597-021-00832-y" title="OutFin, a multi-device and multi-modal dataset for outdoor localization based on the fingerprinting approach | Scientific Data" rel="external nofollow noopener" target="_blank">nature.com</a>, <a href="https://www.nature.com/articles/s41597-021-00832-y?utm_source=chatgpt.com" title="OutFin, a multi-device and multi-modal dataset for outdoor ... - Nature" rel="external nofollow noopener" target="_blank">nature.com</a>)</td> <td>122 outdoor sites, dense fingerprints (~5 GB) (<a href="https://www.nature.com/articles/s41597-021-00832-y" title="OutFin, a multi-device and multi-modal dataset for outdoor localization based on the fingerprinting approach | Scientific Data" rel="external nofollow noopener" target="_blank">nature.com</a>)</td> <td>Outdoor-only (good negative set)</td> <td> <em>RF + sensor fusion</em> for GNSS-denied environments</td> </tr> <tr> <td><strong>Opportunity++ (2021)</strong></td> <td>72 sensors: body IMUs, object IMUs, ambient switches; video; <em>no light, no audio</em> (<a href="https://www.opportunity-project.eu/showcase.html?utm_source=chatgpt.com" title="Showcase | Opportunity" rel="external nofollow noopener" target="_blank">opportunity-project.eu</a>, <a href="https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2021.792065/full?utm_source=chatgpt.com" title="Opportunity++: A Multimodal Dataset for Video- and Wearable ..." rel="external nofollow noopener" target="_blank">frontiersin.org</a>)</td> <td>10 modalities, &gt;4 GB raw time-series/video</td> <td>Primarily indoor</td> <td> <em>Wearable + ambient devices</em>; discuss aligning heterogeneous rates</td> </tr> </tbody> </table> <h1 id="preparing-the-data-for-a-model">Preparing the data for a model</h1> <p>As you can probably imagine, most time-series datasets are not entirely the cleanest. They’re also so information dense that its hard to apply the entire dataset for a model. This section, therefore, talks about the Lets start with the basics of the <code class="language-plaintext highlighter-rouge">ExtraSensory</code> dataset.</p> <ol> <li>It is a dataset from 60 users.</li> <li>Participants were asked to install a companion app on their phone.</li> <li>Time sampling was every 20 seconds for every minute. Data is boxed by the minute on the dataset.</li> <li>Participants were given an easy method to report the state that they were in (walking, driving, indoors, outdoors, etc.)</li> </ol> <p>Therefore, by definition the dataset is a multi-variate time-series dataset. Thinking about <a href="http://www.doorstep.ai" rel="external nofollow noopener" target="_blank">doorstep.ai</a>’s value proposition of providing precise dropoff location accuracy, I decided to focus on whether or not one could identify whether or not the delivery driver was outdoors or indoors. Ostensibly, this is a binary classification problem.</p> <h2 id="downloading-the-dataset">Downloading the dataset</h2> <p>A full guide to how I download the dataset is in the repository <a href="https://github.com/ajay-bhargava/extrasensory/blob/main/src/extrasensory/download/provision.py" rel="external nofollow noopener" target="_blank">here</a>. I make extensive use of <code class="language-plaintext highlighter-rouge">modal</code> because I love their serverless GPU platform oh so much. I will gush over them throughout this blog post so if you’re a <code class="language-plaintext highlighter-rouge">modal</code> hater then you can stop reading now.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CREDENTIALS</span> <span class="o">=</span> <span class="n">modal</span><span class="p">.</span><span class="n">Secret</span><span class="p">.</span><span class="nf">from_name</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">aws-secret</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">environment_name</span><span class="o">=</span><span class="sh">"</span><span class="s">main</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">required_keys</span><span class="o">=</span><span class="p">[</span>
        <span class="sh">"</span><span class="s">AWS_ACCESS_KEY_ID</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">AWS_SECRET_ACCESS_KEY</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">modal</span><span class="p">.</span><span class="nc">App</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-download</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">download_image</span><span class="p">,</span>
<span class="p">)</span>

<span class="nd">@app.function</span><span class="p">(</span>
    <span class="n">volumes</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">/mnt</span><span class="sh">"</span><span class="p">:</span> <span class="n">modal</span><span class="p">.</span><span class="nc">CloudBucketMount</span><span class="p">(</span>
            <span class="n">bucket_name</span><span class="o">=</span><span class="n">BUCKET_NAME</span><span class="p">,</span>
            <span class="n">secret</span><span class="o">=</span><span class="n">CREDENTIALS</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">},</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">download_extrasensory_data</span><span class="p">():</span>
    <span class="bp">...</span>
</code></pre></div></div> <p>Out of principle when I’m working with machine learning datasets I try not to have anything downloaded to my computer. This is because of potential data leakage and security concerns on top of the fact that these datasets are freaking massive and as such are not a good candidate to just hold in my computer at all times. Fortunately with <code class="language-plaintext highlighter-rouge">modal</code> I can just mount any old S3 bucket as a path and then use it as a local path. This is pretty convenient.</p> <h2 id="discovering-temporal-windows">Discovering temporal windows</h2> <h3 id="intuition">Intuition</h3> <p>This dataset is a semi-continuous log of the phone’s sensors in multiple different contexts. If you were to just chuck all the data into a model, you’d likely have poor discrimination between the <code class="language-plaintext highlighter-rouge">outdoor</code> and <code class="language-plaintext highlighter-rouge">indoor</code> class because there are so many additional contexts that the person is in. For example a person can be outside but also inside a car. Indeed this is captured by the author and can be better described in the <a href="https://www.youtube.com/watch?v=2cuhvEQZ_sI&amp;themeRefresh=1" rel="external nofollow noopener" target="_blank">Youtube</a> video linked to this dataset. Furthermore, this graph clearly articulates all the captured contexts for a typical user in the study.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/extrasensory-data-scope-480.webp 480w,/assets/img/extrasensory-data-scope-800.webp 800w,/assets/img/extrasensory-data-scope-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/extrasensory-data-scope.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Diving a bit deeper, the dataset contains temporal windows of the following phone sensors:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="caption">Accelerometer</div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/example_phone_acc-480.webp 480w,/assets/img/example_phone_acc-800.webp 800w,/assets/img/example_phone_acc-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/example_phone_acc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <div class="caption">Audio</div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/example_audio-480.webp 480w,/assets/img/example_audio-800.webp 800w,/assets/img/example_audio-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/example_audio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>I had a thought. What if i narrow the context of the data to be used in the model to the point in the dataset where the person is transitioning between the outside and inside (e.g. immediately upon entering an enclosed building or leaving it?) This would increase the relevant number of samples to a value twice the number of users in the dataset.</p> \[N_{\text{after}} = 2\sum_{u=1}^{U} S_u = 2\,N_{\text{before}}\] <p>The number of samples after the temporal window is narrowed is twice the number of samples before the temporal window is narrowed. Implementing this in python required a bit of work but ultimately resulted in the desired effect, I had more temporal windows to work with that narrowly described the transition between the outside and inside class within a specified window of <code class="language-plaintext highlighter-rouge">window_size</code>.</p> <h3 id="implementation">Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mark_transition_windows</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">col_inside</span><span class="p">,</span> <span class="n">col_outside</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">label_names</span><span class="p">,</span> <span class="n">new_label_names</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Mark transition windows in the data matrix X for transitions between two mutually exclusive labels.

    This function adds three new boolean columns to X:
        - The first new column is True for samples in a window before each transition.
        - The second new column is True for samples in a window after each transition.
        - The third new column is True at the exact transition point.
    The names for these new columns are provided by new_label_names and appended to label_names.

    Parameters
    ----------
    X : np.ndarray
        The input data matrix of shape (n_samples, n_features).
    col_inside : int
        The column index in X corresponding to the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> label (binary: 0 or 1).
    col_outside : int
        The column index in X corresponding to the </span><span class="sh">"</span><span class="s">outside</span><span class="sh">"</span><span class="s"> label (binary: 0 or 1).
    window_size : int
        The number of samples before and after the transition to mark as the window.
    label_names : list of str
        The list of existing label/feature names (length should match X.shape[1]).
    new_label_names : list of str
        The list of 3 names for the new columns (e.g., [</span><span class="sh">"</span><span class="s">left_window</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">right_window</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">transition</span><span class="sh">"</span><span class="s">]).

    Returns
    -------
    X_new : np.ndarray
        The augmented data matrix with three additional boolean columns indicating:
            - Before transition window
            - After transition window
            - At transition point
    label_names_new : list of str
        The updated list of label/feature names with the new column names appended.

    Notes
    -----
    - Assumes that the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> and </span><span class="sh">"</span><span class="s">outside</span><span class="sh">"</span><span class="s"> columns are mutually exclusive (never both 1).
    - Transitions are detected as changes in the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> label.
    - If a transition window would extend beyond the bounds of X, it is skipped.

    Example
    -------
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="n">X_aug</span><span class="p">,</span> <span class="n">label_names_aug</span> <span class="o">=</span> <span class="nf">mark_transition_windows</span><span class="p">(</span>
    <span class="p">...</span>     <span class="n">X</span><span class="p">,</span> <span class="n">col_inside</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col_outside</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">...</span>     <span class="n">label_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="p">...</span>     <span class="n">new_label_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">left_window</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">right_window</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">transition</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">...</span> <span class="p">)</span>
    <span class="sh">"""</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">old_cols</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)])</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">col_inside</span><span class="p">]</span>
    <span class="n">outside</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">col_outside</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">all</span><span class="p">((</span><span class="n">inside</span> <span class="o">+</span> <span class="n">outside</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="sh">"</span><span class="s">Labels must be mutually exclusive</span><span class="sh">"</span>
    <span class="n">inside_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">inside</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">prepend</span><span class="o">=</span><span class="n">inside</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">inside_diff</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">window_size</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">X_new</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">idx</span><span class="p">,</span> <span class="n">old_cols</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>         <span class="c1"># Before
</span>        <span class="n">X_new</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="n">old_cols</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>       <span class="c1"># After
</span>        <span class="n">X_new</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">old_cols</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>             <span class="c1"># At transition
</span>    <span class="n">label_names_new</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">label_names</span><span class="p">)</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">new_label_names</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_new</span><span class="p">,</span> <span class="n">label_names_new</span>
</code></pre></div></div> <h3 id="visual-description">Visual Description</h3> <p>Visually this could also be described as the graph below. The transition points are marked in purple with the before and after window of ~5 minutes marked in red and green respectively. The analysis isn’t completely perfect but it is directionally correct for the length of time I worked on this for a proof of concept.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-480.webp 480w,/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-800.webp 800w,/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h2 id="cleaning-and-preparing-the-dataset">Cleaning and preparing the dataset</h2> <p>I then ran a clean of the potential <code class="language-plaintext highlighter-rouge">np.nan</code> and <code class="language-plaintext highlighter-rouge">np.inf</code> values that would most defintiely interfere with a multi-variate classification model. This can be found in the github repository <a href="https://github.com/ajay-bhargava/extrasensory/blob/f9998b04509ba6cf2c96f66bc415044f2096ee96/src/extrasensory/prepare/utils/massage.py#L126-L189" rel="external nofollow noopener" target="_blank">here</a>. Running the code below resulted in a labels array that described the classes of the data. The final shape of the labels array was <code class="language-plaintext highlighter-rouge">(4293,)</code>. The final shape of the dataset was <code class="language-plaintext highlighter-rouge">(4293, 5, 225)</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside is 0, Outside is 1
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">inside_windows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span> 
    <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">outside_windows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span>
<span class="p">)</span> 
</code></pre></div></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Ajay Bhargava Ph.D.. Last updated: June 17, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>