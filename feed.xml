<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ajay-bhargava.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ajay-bhargava.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-02T15:21:07+00:00</updated><id>https://ajay-bhargava.github.io/feed.xml</id><title type="html">Ajay Bhargava Ph.D.</title><subtitle>Ajay&apos;s Professional Website </subtitle><entry><title type="html">One more AI agent and you’ll be Clairvoyant, I promise.</title><link href="https://ajay-bhargava.github.io/blog/2025/clairvoyant/" rel="alternate" type="text/html" title="One more AI agent and you’ll be Clairvoyant, I promise."/><published>2025-10-01T16:00:00+00:00</published><updated>2025-10-01T16:00:00+00:00</updated><id>https://ajay-bhargava.github.io/blog/2025/clairvoyant</id><content type="html" xml:base="https://ajay-bhargava.github.io/blog/2025/clairvoyant/"><![CDATA[<blockquote> <p><strong>Clairvoyance</strong> /ˌklerˈvoiən(t)s/ <em>noun</em> The supposed faculty of perceiving things or events beyond normal sensory contact.</p> </blockquote> <p>I’m fascinated by the idea of clairvoyance. The word itself literally means “clear-seeing.” In modern usage, it’s tied to paranormal or spiritual vision, and pop culture often reduces it to pre-cognition. Ancient texts, however, describe clairvoyance as God’s domain: retro-cognition and extrasensory perception. From the Vedas to the Bible to Islamic writings, humanity has been handed a playbook for aspiring to reflect the divine. Who could’ve guessed that thousands of years later the reconstruction of Jehovah’s sand in the form of Jensen’s GPU would be the means to make it all possible? <em>Yaas queen</em>, we’re talking AI wrappers, let’s gooo!</p> <h2 id="what-did-i-make">What did I make?</h2> <p>Clairvoyant is a real-time voice transcription and intelligent context-aware assistant. It captures audio, transcribes it as text, processes it through multiple cascades of AI agents, and then provides personalized responses using multiple AI tools to pass informative context back to the user’s wave-guide optics HUD. During this process, Clairvoyant simultaneously captures all transcribed audio and adds it to memory. Over time, the user can <a href="https://youtu.be/JrGYLNHfUwE?si=GDZbjTGA7rdpwqGG">achieve 20/20 hindsight</a></p> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/10/948c117ffc7e3806075f079f43823df8.gif" alt="image"/></p> <p>and recall from their memories things that matter to them. I’ve found it useful to remember things that I said, things that I’ve heard, and reflection on the questions I’ve asked about over a growing number of weeks and months. In short, I’m getting closer to my vision of having E.D.I.T.H, JARVIS, and Cortana.</p> <blockquote> <p>💬 <strong>Show me the code</strong></p> <p>🤖 You’re absolutely right, <a href="https://github.com/ajay-bhargava/clairvoyant">here’s the code</a>.</p> </blockquote> <h2 id="my-software-design-philosophy-in-ai-glasses">My software design philosophy in AI Glasses</h2> <p>Clairvoyant is my first real flex at making a highly opinionated codebase following my time at the <a href="https://fractalbootcamp.com">Fractal Accelerator of NYC</a>. <a href="https://www.amazon.com/Grokking-Simplicity-software-functional-thinking/dp/1617296201">Grokking Simplicity</a> and <a href="https://pragprog.com/titles/tpp20/the-pragmatic-programmer-20th-anniversary-edition/">Pragmatic Programmer</a>teach core fundamentals of designing great code and are required reading in the accelerator. I’ve felt, however, that with traditional applications of programming (e.g. ML code or Full-stack Engineering) you’ve got to obey the primitives or abstractions that others <a href="https://www.reddit.com/r/nextjs/comments/1ag76t0/nextjs_14_vs_pages_meme/">have designed for you</a>. Because of this, you’re not really thinking about the “business logic” and rather thinking about “integration patterns” and that isn’t terribly satisfying.</p> <p>What’s fun in AR/AI glasses is that the whole field is still very greenfield. While there are growing codebases to support hardware, the design philosophy of how software programs should be written isn’t really there yet. I think that the extensibility of software in the AI glasses domain could be greatly enhanced <em>if</em> we all started forming (and writing) opinions and thoughts on how code should “work” in this paradigm and then debated them. So, without further ado, here are some design principles I learned through real-world testing the Clairvoyant app (and building AR glasses software) for the last 3 months.</p> <h3 id="write-apis-for-hardware-access-boost-iteration-speed">Write API’s for hardware access, boost iteration speed.</h3> <p>I’ve worked with the Brilliant Frames in the past. In fact, I was the <a href="https://x.com/0x1F9ED/status/1936792737774288967">fan favorite</a> in a Hackathon in NYC because of my ability to create apps that mattered quickly on that platform. That being said, creating upon the <a href="https://github.com/CitizenOneX/frame_vision_api">Flutter</a> repository template didn’t feel super intuitive. It “<em>worked</em>” because the majority of the application I created was a backend <a href="https://github.com/with-context-engine/context-personhood">API hosted in Convex</a> and this did all the heavy lifting. It received image data from the app which knew about the API endpoint and then the API endpoint logic handled most of the image interpretation. Now, smart readers will know that Convex is dripping <a href="https://stack.convex.dev/how-convex-works">WebSockets</a>so that you can get insanely low latency transactions between your service and the client. In my case, the API endpoint was designed to receive image data, but it could’ve easily been adapted to audio and haptic/IMU data too.</p> <h3 id="stay-away-from-swift-use-platforms-that-house-your-ai-glasses-app-instead">Stay away from Swift. Use platforms that house your AI glasses app instead.</h3> <p>The underlying hardware code had been graciously written by <a href="https://docs.brilliant.xyz/frame/frame-sdk-lua/">the Brilliant team</a>so it was only a matter of effort to get the full glasses integration. The hard part wasn’t actually developing the API endpoint. What slowed me down wasn’t the API or the hardware. It was wrestling with Flutter, packaging builds in Xcode, pushing to TestFlight, and debugging code that didn’t really matter to what I was building. Between July and August 2025 I thought for a hot minute, whoa, I think there should be a startup to write infrastructure “glue” that standardizes connections between hardware (client) and server. Turns out, however, <a href="https://docs.mentra.glass/quickstart">MentraOS</a> (formerly AugmentOS) has already existed for years and did just that! The primitive to deal with audio, for example, is high touch enough that you get a VAD model to transcribe the audio into text without having to worry about doing that yourself. They use <a href="https://soniox.com">Soniox</a> as their audio translation provider. This is all it takes, for example, to get the spoken text from the user or anyone around him as he/she is wearing AR glasses. With MentraOS, you’re not building apps for the Apple App Store, you’re building apps for the MentraOS app store. I’m still in awe with how Apple approved this.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Subscribe to transcription events</span>
<span class="nx">appSession</span><span class="p">.</span><span class="nx">events</span><span class="p">.</span><span class="nf">onTranscription</span><span class="p">((</span><span class="nx">data</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
  <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="s2">`Transcription: </span><span class="p">${</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
  <span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="s2">`Final: </span><span class="p">${</span><span class="nx">data</span><span class="p">.</span><span class="nx">isFinal</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>

  <span class="k">if </span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">isFinal</span><span class="p">)</span> <span class="p">{</span>
  <span class="p">}</span>
<span class="p">});</span>
</code></pre></div></div> <h3 id="mind-your-latency">Mind your latency</h3> <p>In my early prototypes of Clairvoyant, I would wantonly call AI tools and have my daily audio transcription acted on as soon as I’d be done speaking. Depending on the latency of the AI tool, this would either be super annoying, or super frustrating. Allow me to how I think about latency and control-flow in the age of AI glasses.</p> <p><strong>“Be Fast” expectations</strong></p> <p>I have a theory. The closer software gets to our eyeballs, the less patience people have to it taking too long to “work”. Two people staring at an iPhone loading a webpage? Totally fine! A person waiting a second on his glasses for the weather, or, the subsequent return to an answer to a query about the “role of context engineering in LLM’s”? Preposterous, its taking too god-damn long!</p> <p><strong>“Be Slow” expectations</strong></p> <p>Is the text appearing too fast? You’ll hear your inner monologue say: “What’s going on here? Is this thing even working?”. As a developer, even you’ll have a pause for concern: Hang on I’m not sure this answer really searched the web, or utilized agentic memory correctly. Gleaning the pace at which users will “expect” answers is an acquired taste.</p> <p>Since I’ve been wearing Clairvoyant for about a month now I’ve had some thoughts as to how to address latency. How do we strike the right balance?</p> <h4 id="dont-listen-to-everything-you-hear">Don’t listen to everything you hear</h4> <p>Right off the bat we have to start with preventing erroneous audio chunks from making their way into the <code class="language-plaintext highlighter-rouge">handleTranscription()</code> flow. This allows you to reserve your AI inference server for queries that matter. This is accomplished by three methods:</p> <ol> <li> <p>Making sure that the VAD model (Soniox) is notifying the application that the speech is done. Soniox claims to use intonation, breath, and other audio characteristics to determine the completion of speech. Since I haven’t invested time and energy into developing better Mel Frequency Cepstral Coefficents tooling (see <a href="https://www.kaggle.com/code/ilyamich/mfcc-implementation-and-tutorial">here</a> for more information on that) I’m just going to rely on the out of the box service for now.</p> </li> <li> <p>Making sure that the duration of the transcribed speech isn’t a sentence fragment or a spurious audio fragment. You’ll be surprised how sometimes sensitive or not sensitive (e.g. finicky) microphones can be. With that there are spurious detections of speech, spurious transcriptions and those happen to be quite short. Eliminating them is a good idea.</p> </li> <li> <p>Sometimes you’ll ask one or two or maybe three successive questions one after another. Or maybe you’re in a crowded room of excited people who are rapid firing questions at you. A simple <code class="language-plaintext highlighter-rouge">rateLimiter</code> is useful for preventing too many queries and thus too many displays on the glasses all at once.</p> </li> </ol> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">protected</span> <span class="nx">override</span> <span class="k">async</span> <span class="nf">onSession</span><span class="p">(</span><span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">):</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="k">void</span><span class="o">&gt;</span> <span class="p">{</span>
		<span class="kd">const</span> <span class="p">[</span><span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">]</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">initializeMemory</span><span class="p">();</span>

		<span class="nx">session</span><span class="p">.</span><span class="nx">events</span><span class="p">.</span><span class="nf">onTranscription</span><span class="p">(</span><span class="k">async </span><span class="p">(</span><span class="nx">data</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
			<span class="c1">// [1] If its not a final utterance, skip</span>
			<span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">data</span><span class="p">.</span><span class="nx">isFinal</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>

			<span class="c1">// [2] If the audio segment causing this transcription is too short, skip</span>
			<span class="k">if </span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">duration</span><span class="p">)</span> <span class="p">{</span>
				<span class="k">if </span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">duration</span> <span class="o">&lt;</span> <span class="mi">200</span><span class="p">)</span> <span class="p">{</span>
					<span class="k">return</span><span class="p">;</span>
				<span class="p">}</span>
			<span class="p">}</span>

			<span class="c1">// [3] If the question rate limiter is triggered, skip</span>
			<span class="k">if </span><span class="p">(</span><span class="k">this</span><span class="p">.</span><span class="nx">questionRateLimiter</span><span class="p">.</span><span class="nf">shouldSkip</span><span class="p">(</span><span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">,</span> <span class="dl">"</span><span class="s2">Clairvoyant</span><span class="dl">"</span><span class="p">))</span> <span class="p">{</span>
				<span class="k">return</span><span class="p">;</span>
			<span class="p">}</span>

			<span class="c1">// Handle the transcription</span>
			<span class="k">await</span> <span class="nf">handleTranscription</span><span class="p">(</span><span class="nx">data</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
		<span class="p">});</span>
	<span class="p">}</span>
</code></pre></div></div> <h4 id="use-lightning-fast-ai-models-for-your-ai-agents">Use lightning fast AI models for your AI agents</h4> <table> <tbody> <tr> <td>There are 6 agents that handle transcribed text. Iterating through different providers and prompts has been super delightful thanks to <a href="https://boundaryml.com">BAML</a>(more on this [[#Use lightning fast AI models for your AI agents</td> <td>here]]).</td> </tr> </tbody> </table> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>baml_src/
├── route.baml          <span class="c"># Main routing logic and route definitions</span>
├── weather.baml        <span class="c"># Weather response agent</span>
├── search.baml         <span class="c"># Web search result agent  </span>
├── maps.baml           <span class="c"># Location/maps response agent</span>
├── recall.baml         <span class="c"># Memory recall agent</span>
├── answer.baml         <span class="c"># General knowledge agent</span>
├── clients.baml        
└── generators.baml     
</code></pre></div></div> <p>Crucially, however, its necessary to choose the fastest and lowest latency AI models to interpret text. Fortunately you can do this via BAML in the <code class="language-plaintext highlighter-rouge">clients.baml</code> settting. I ended up choosing <code class="language-plaintext highlighter-rouge">GPT-OSS-20B</code> and/or <code class="language-plaintext highlighter-rouge">GPT-OSS-120B</code> depending on the complexity or ambiguity of the query that was sent to that particular agent.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>client&lt;llm&gt; Groq {
  provider openai-generic
  options {
    base_url "https://api.groq.com/openai/v1"
    api_key env.GROQ_API_KEY
    model "openai/gpt-oss-20b"
  }
}

client&lt;llm&gt; GroqHeavy {
  provider openai-generic
  options {
    base_url "https://api.groq.com/openai/v1"
    api_key env.GROQ_API_KEY
    model "openai/gpt-oss-120b"
  }
}
</code></pre></div></div> <p>Outside of those AI models sped up on <a href="https://groq.com">Groq</a> using Groq’s <a href="https://groq.com/lpu-architecture">LPU</a>’s I never dabble in any of the current generation models because they’re so god awfully slow. The only model that actually feels fast that isn’t hosted by Groq is <code class="language-plaintext highlighter-rouge">openai/gpt-4o-mini</code> and I provide that agent with highly structured weather data to interpret weather requests.</p> <h4 id="use-fast-ai-tools">Use fast AI tools</h4> <p>Every tool that touches text must be fast. This is because you’re operating in so many compounding latency constraints. Audio transcription: latency. The AI agent that routes your query to the other AI agent: latency. The receipt of the text back to the glasses following the tool interpretation agent massaging returned tool data: latency.</p> <p>Latency is everywhere, and so selecting the right tooling frameworks that deliver what you need fast without fail is of the highest concern. With that in mind, here are the tool providers I used for each data source:</p> <table> <thead> <tr> <th>Tool</th> <th>Provider</th> <th>Latency (ms)</th> </tr> </thead> <tbody> <tr> <td>Web Search</td> <td><a href="https://www.tavily.com">Tavily</a></td> <td>~600ms</td> </tr> <tr> <td>Maps Search</td> <td>Google Maps (<a href="https://developers.google.com/maps/documentation/places/web-service/text-search">Text Search API</a>)</td> <td>~400ms</td> </tr> <tr> <td>Weather</td> <td><a href="https://openweathermap.org">OpenWeatherMap</a></td> <td>~300ms</td> </tr> <tr> <td>Memory</td> <td><a href="https://plasticlabs.ai/#main">Plastic Labs</a></td> <td>~1200ms</td> </tr> </tbody> </table> <h3 id="temper-user-frustration-of-latency-with-loading-hints">Temper user frustration of latency with loading hints</h3> <p>I know it sounds really simple, but a really easy design choice you can make on the Even Realities G1’s is to provide the user with text while another function is awaiting return. Here’s what that looks like:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="kd">type</span> <span class="nx">AppSession</span><span class="p">,</span> <span class="nx">ViewType</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">@mentra/sdk</span><span class="dl">"</span><span class="p">;</span>

<span class="k">export</span> <span class="k">async</span> <span class="kd">function</span> <span class="nf">showTextDuringOperation</span><span class="o">&lt;</span><span class="nx">T</span><span class="o">&gt;</span><span class="p">(</span>
	<span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">,</span>
	<span class="nx">loadingText</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
	<span class="nx">doneText</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
	<span class="nx">errorText</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
	<span class="nx">asyncOperation</span><span class="p">:</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="nx">T</span><span class="o">&gt;</span><span class="p">,</span>
	<span class="nx">options</span><span class="p">:</span> <span class="p">{</span>
		<span class="nl">view</span><span class="p">?:</span> <span class="nx">ViewType</span><span class="p">;</span>
		<span class="nl">clearDurationMs</span><span class="p">?:</span> <span class="kr">number</span><span class="p">;</span>
	<span class="p">}</span> <span class="o">=</span> <span class="p">{},</span>
<span class="p">):</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="nx">T</span><span class="o">&gt;</span> <span class="p">{</span>
	<span class="kd">const</span> <span class="p">{</span> <span class="nx">view</span> <span class="o">=</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span> <span class="nx">clearDurationMs</span> <span class="o">=</span> <span class="mi">5000</span> <span class="p">}</span> <span class="o">=</span> <span class="nx">options</span><span class="p">;</span>

	<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="nx">loadingText</span><span class="p">,</span> <span class="p">{</span>
		<span class="nx">view</span><span class="p">,</span>
		<span class="na">durationMs</span><span class="p">:</span> <span class="mi">30000</span><span class="p">,</span>
	<span class="p">});</span>

	<span class="k">try</span> <span class="p">{</span>
		<span class="kd">const</span> <span class="nx">result</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">asyncOperation</span><span class="p">();</span>

		<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="nx">doneText</span><span class="p">,</span> <span class="p">{</span>
			<span class="nx">view</span><span class="p">,</span>
			<span class="na">durationMs</span><span class="p">:</span> <span class="nx">clearDurationMs</span><span class="p">,</span>
		<span class="p">});</span>

		<span class="k">return</span> <span class="nx">result</span><span class="p">;</span>
	<span class="p">}</span> <span class="k">catch </span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
		<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="nx">errorText</span><span class="p">,</span> <span class="p">{</span>
			<span class="nx">view</span><span class="p">,</span>
			<span class="na">durationMs</span><span class="p">:</span> <span class="nx">clearDurationMs</span><span class="p">,</span>
		<span class="p">});</span>
		<span class="k">throw</span> <span class="nx">error</span><span class="p">;</span>
	<span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div> <p>and here’s what it looks like in operation:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">const</span> <span class="nx">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">showTextDuringOperation</span><span class="p">(</span>
	<span class="nx">session</span><span class="p">,</span>
	<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Trying to remember...</span><span class="dl">"</span><span class="p">,</span>
	<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Got it!</span><span class="dl">"</span><span class="p">,</span>
	<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Couldn't remember!</span><span class="dl">"</span><span class="p">,</span>
	<span class="p">()</span> <span class="o">=&gt;</span> <span class="nx">diatribePeer</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="nx">textQuery</span><span class="p">),</span>
<span class="p">);</span>
</code></pre></div></div> <p>The reason why this is so helpful is that some applications are indeed slow and that’s because they’re grooming over a ton of contextual data to sound relevant. By displaying something to the user during the load of another screen, you quell the user’s frustrations. The real trick is, I think, matching the user’s expectations with the reality for what the AI tool or AI agent is intending to do. Calling on your personal memory layer? “Trying to Remember.”. Looking up something on the internet? “Searching the web.”. Fun cues to make the user feel engaged with a real assistant are always helpful.</p> <h2 id="advice-for-vibe-coding-a-personal-assistant">Advice for vibe coding a personal assistant</h2> <p>In designing Clairvoyant, I first did a lot of nonsense. I had page long vibe codes, tools mixed in with agents, and just more slop than you could think of. This was actually really useful because it let me sink into the trough of enlightenment as to how I’d want to make repeatable code modules that could be defined and refined. Here are some of the fundamentals I laid down having spent far too many tokens messing around.</p> <h4 id="create-a-router-that-an-llm-can-expand-upon">Create a router that an LLM can expand upon</h4> <p>One of the early design challenges I faced was how to create an extensible foundation upon which I could build additional tools without having to constantly rework the software architecture. The <code class="language-plaintext highlighter-rouge">switch</code> statement and good types to the rescue!</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">export</span> <span class="k">async</span> <span class="kd">function</span> <span class="nf">handleTranscription</span><span class="p">(</span>
	<span class="nx">data</span><span class="p">:</span> <span class="nx">TranscriptionData</span><span class="p">,</span>
	<span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">,</span>
	<span class="nx">memorySession</span><span class="p">:</span> <span class="nx">Session</span><span class="p">,</span>
	<span class="nx">peers</span><span class="p">:</span> <span class="nx">Peer</span><span class="p">[],</span>
<span class="p">)</span> <span class="p">{</span>
	<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[Clairvoyant] Transcription: </span><span class="p">${</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
	<span class="kd">const</span> <span class="nx">routing</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">b</span><span class="p">.</span><span class="nc">Route</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">);</span>
	<span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">routing</span><span class="p">.</span><span class="nx">routing</span><span class="p">)</span> <span class="p">{</span>
		<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">warn</span><span class="p">(</span><span class="s2">`[Clairvoyant] No routing decision made. Resetting...`</span><span class="p">);</span>
		<span class="k">return</span><span class="p">;</span>
	<span class="p">}</span>
	<span class="k">switch </span><span class="p">(</span><span class="nx">routing</span><span class="p">.</span><span class="nx">routing</span><span class="p">)</span> <span class="p">{</span>
		<span class="k">case</span> <span class="nx">Router</span><span class="p">.</span><span class="nx">WEATHER</span><span class="p">:</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[Clairvoyant] Weather route: starting async flow`</span><span class="p">);</span>
			<span class="k">void</span> <span class="nf">startWeatherFlow</span><span class="p">(</span><span class="nx">session</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>

		<span class="k">case</span> <span class="nx">Router</span><span class="p">.</span><span class="nx">MAPS</span><span class="p">:</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[Clairvoyant] Maps route: starting async flow`</span><span class="p">);</span>
			<span class="k">void</span> <span class="nf">startMapsFlow</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>

		<span class="k">case</span> <span class="nx">Router</span><span class="p">.</span><span class="nx">WEB_SEARCH</span><span class="p">:</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
				<span class="s2">`[Clairvoyant] Web search route: starting async flow`</span><span class="p">,</span>
			<span class="p">);</span>
			<span class="k">void</span> <span class="nf">startWebSearchFlow</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>

		<span class="k">case</span> <span class="nx">Router</span><span class="p">.</span><span class="nx">KNOWLEDGE</span><span class="p">:</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[Clairvoyant] Routing: Starting knowledge flow`</span><span class="p">);</span>
			<span class="k">void</span> <span class="nf">startKnowledgeFlow</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>

		<span class="k">case</span> <span class="nx">Router</span><span class="p">.</span><span class="nx">MEMORY_RECALL</span><span class="p">:</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
				<span class="s2">`[Clairvoyant] Memory Recall route: starting async flow`</span><span class="p">,</span>
			<span class="p">);</span>
			<span class="k">void</span> <span class="nc">MemoryRecall</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>

		<span class="nl">default</span><span class="p">:</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
				<span class="s2">`[Clairvoyant] Memory Insertion route: starting async flow`</span><span class="p">,</span>
			<span class="p">);</span>
			<span class="k">void</span> <span class="nc">MemoryCapture</span><span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">text</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>These are defined by an <code class="language-plaintext highlighter-rouge">Enum</code> in BAML that allows an LLM (in this case <code class="language-plaintext highlighter-rouge">gpt-oss-120b</code>) to intelligently decide how to route the user query to the right tool call to get the user’s query resolved.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>enum Router {
    WEATHER @description("Current or upcoming weather questions for a specific place.")
    WEB_SEARCH @description("News, current events, facts that change over time such as political events, or topics not obviously location-based.")
    MAPS @description("Finding nearby businesses, restaurants, addresses, or directions.")
    KNOWLEDGE @description("General knowledge that does not fit into other categories.")
    MEMORY_RECALL @description("Questions about the user's memory and personal history, personal preferences, personal opinions, goals, information about the user, or anything that is not a fact.")
}

class RoutingBehavior {
    origin string @description("Echo of the user's input text verbatim.")
    routing Router
}

function Route(text: string) -&gt; RoutingBehavior {
    client "GroqHeavy"
    prompt #"
        You are a routing assistant. Pick the best router for the user's request.
         
        
    "#
}
</code></pre></div></div> <p>Now, one thing I will say is relying on an LLM to route natural language queries can be fraught with a ton of error, but, fortunately, with BAML you can just <em>test</em> your AI agents!</p> <h5 id="use-baml-for-defining-your-ai-agents-thats-it">Use BAML for defining your AI agents. That’s it.</h5> <p>I cannot stress this enough. The ability to have clean types that match the structured output from an LLM means that you can develop tests. I use <code class="language-plaintext highlighter-rouge">baml</code> included test suite to test a battery of queries every time I change my system or assistant prompt. Even if there were to be a model change, I flag on deployment.</p> <pre><code class="language-baml">test test_weather {
    functions [Route]
    args {
        text "What is the weather in San Francisco?"
    }
    @@assert( )
}

test test_web_search {
    functions [Route]
    args {
        text "Who is the current president of the United States?"
    }
    @@assert( )
}

test test_maps {
    functions [Route]
    args {
        text "Find me a ramen restaurant near Union Square."
    }
    @@assert( )
}

test test_knowledge {
    functions [Route]
    args {
        text "What is the capital of France?"
    }
    @@assert( )
}

test test_knowledge_2 {
    functions [Route]
    args {
        text "What is the purpose of mitochondria?"
    }
    @@assert( )
}

test test_memory {
    functions [Route]
    args {
        text "What is my name?"
    }
    @@assert( )
}

test test_memory_2 {
    functions [Route]
    args {
        text "Koyal, what did I eat yesterday?"
    }
    @@assert( )
}
</code></pre> <h4 id="create-a-separation-of-concerns-for-your-tools-with-handlers">Create a separation of concerns for your tools with handlers</h4> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/10/382aca351e6ee17234d581ebc05493b4.png" alt="image" style="max-width: 100%; height: auto; display: block; margin: 0 auto;"/></p> <p>In all the subsequent applications I’ve developed, I’ve maintained the pattern where handlers call tools. Like tools, AI agents are handled by the handler. That way they are constrained by the control flow. See the example below:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">export</span> <span class="k">async</span> <span class="kd">function</span> <span class="nf">startWebSearchFlow</span><span class="p">(</span>
	<span class="nx">query</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
	<span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">,</span>
	<span class="nx">memorySession</span><span class="p">:</span> <span class="nx">Session</span><span class="p">,</span>
	<span class="nx">peers</span><span class="p">:</span> <span class="nx">Peer</span><span class="p">[],</span>
<span class="p">)</span> <span class="p">{</span>
	<span class="kd">const</span> <span class="nx">runId</span> <span class="o">=</span> <span class="nb">Date</span><span class="p">.</span><span class="nf">now</span><span class="p">();</span>
	<span class="nx">webSearchRunIds</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="nx">session</span><span class="p">,</span> <span class="nx">runId</span><span class="p">);</span>

	<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
		<span class="s2">`[startWebSearchFlow] Starting web search flow for query: </span><span class="p">${</span><span class="nx">query</span><span class="p">}</span><span class="s2">`</span><span class="p">,</span>
	<span class="p">);</span>
	
	<span class="c1">// [1] Try the WebSearchTool</span>
	<span class="k">try</span> <span class="p">{</span>
		<span class="kd">const</span> <span class="nx">searchResults</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">showTextDuringOperation</span><span class="p">(</span>
			<span class="nx">session</span><span class="p">,</span>
			<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">S: Searching the web...</span><span class="dl">"</span><span class="p">,</span>
			<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">S: Found it!</span><span class="dl">"</span><span class="p">,</span>
			<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">S: Couldn't search the web.</span><span class="dl">"</span><span class="p">,</span>
			<span class="p">()</span> <span class="o">=&gt;</span> <span class="nf">performWebSearch</span><span class="p">(</span><span class="nx">query</span><span class="p">),</span>
		<span class="p">);</span>
		
		<span class="c1">// [2] Capture in Memory Tool</span>
		<span class="k">await</span> <span class="nc">MemoryCapture</span><span class="p">(</span><span class="nx">query</span><span class="p">,</span> <span class="nx">session</span><span class="p">,</span> <span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">);</span>

		<span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">searchResults</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">throw</span> <span class="k">new</span> <span class="nc">Error</span><span class="p">(</span><span class="dl">"</span><span class="s2">No response from web search</span><span class="dl">"</span><span class="p">);</span>
		<span class="p">}</span>

		<span class="k">if </span><span class="p">(</span><span class="nx">webSearchRunIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
				<span class="s2">`[startWebSearchFlow] Web search response arrived for stale request, discarding`</span><span class="p">,</span>
			<span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>
		
		
		<span class="c1">// [3] AI agent handling WebSearchResults</span>
		<span class="kd">const</span> <span class="nx">answerLines</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">b</span><span class="p">.</span><span class="nc">AnswerSearch</span><span class="p">(</span><span class="nx">query</span><span class="p">,</span> <span class="nx">searchResults</span><span class="p">);</span>

		<span class="k">if </span><span class="p">(</span><span class="nx">webSearchRunIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
				<span class="s2">`[startWebSearchFlow] Web search response arrived for stale request, discarding`</span><span class="p">,</span>
			<span class="p">);</span>
			<span class="k">return</span><span class="p">;</span>
		<span class="p">}</span>

		<span class="kd">const</span> <span class="nx">lines</span> <span class="o">=</span> <span class="nx">answerLines</span><span class="p">.</span><span class="nx">results</span><span class="p">[</span><span class="mi">0</span><span class="p">]?.</span><span class="nx">lines</span><span class="p">;</span>
		
		<span class="c1">// [4] Display Text on AI glasses</span>
		<span class="k">if </span><span class="p">(</span><span class="nx">lines</span><span class="p">?.</span><span class="nx">length</span><span class="p">)</span> <span class="p">{</span>
			<span class="k">for </span><span class="p">(</span><span class="kd">let</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">lines</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
				<span class="kd">const</span> <span class="nx">line</span> <span class="o">=</span> <span class="nx">lines</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span>
				<span class="k">if </span><span class="p">(</span><span class="nx">webSearchRunIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
				<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[startWebSearchFlow] Web search result: </span><span class="p">${</span><span class="nx">line</span><span class="p">}</span><span class="s2">`</span><span class="p">);</span>
				<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="s2">`// Clairvoyant\nS: </span><span class="p">${</span><span class="nx">line</span><span class="p">}</span><span class="s2">`</span><span class="p">,</span> <span class="p">{</span>
					<span class="na">view</span><span class="p">:</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span>
					<span class="na">durationMs</span><span class="p">:</span> <span class="mi">3000</span><span class="p">,</span>
				<span class="p">});</span>
				<span class="k">if </span><span class="p">(</span><span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">lines</span><span class="p">.</span><span class="nx">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
					<span class="k">await</span> <span class="k">new</span> <span class="nc">Promise</span><span class="p">((</span><span class="nx">resolve</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nf">setTimeout</span><span class="p">(</span><span class="nx">resolve</span><span class="p">,</span> <span class="mi">3000</span><span class="p">));</span>
				<span class="p">}</span>
			<span class="p">}</span>
		<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span><span class="s2">`[startWebSearchFlow] No lines in answerLines`</span><span class="p">);</span>
		<span class="p">}</span>
	<span class="p">}</span> <span class="k">catch </span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
		<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span>
			<span class="s2">`[startWebSearchFlow] Web search flow error: </span><span class="p">${</span><span class="nc">String</span><span class="p">(</span><span class="nx">error</span><span class="p">)}</span><span class="s2">`</span><span class="p">,</span>
		<span class="p">);</span>

		<span class="k">if </span><span class="p">(</span><span class="nx">webSearchRunIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">===</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span>
				<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">S: Couldn't search the web.</span><span class="dl">"</span><span class="p">,</span>
				<span class="p">{</span>
					<span class="na">view</span><span class="p">:</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span>
					<span class="na">durationMs</span><span class="p">:</span> <span class="mi">3000</span><span class="p">,</span>
				<span class="p">},</span>
			<span class="p">);</span>
		<span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="then-write-agentsmd-andor-cursorrules">Then write <code class="language-plaintext highlighter-rouge">AGENTS.md</code> and/or <code class="language-plaintext highlighter-rouge">./cursor/rules</code></h3> <p>Ok, now the part you’re probably interested in. Surprise, the lesson here is a little boring, tbh. If you want a great time vibe-coding, you’re going to need to have a ton of structure to your code. If you’ve made it this far in the blog you’re probably already aware of this. If you’re skipping around, sorry, you’re going to have to go back and read the first few sections! Its true. When the code was well structured (or at least, structured enough such that there was a clear separation of concerns between the code modules) it was far easier to just tell a code generation model that I needed to integrate yet another tool with an existing handler pattern. Once I wrote the knowledge tool, the search tool and the maps tool were pretty simple. The handlers are all about the same. Memory (to be discussed later) was even easier because it just had to be inserted between query and response generation.</p> <h3 id="accelerate-using-cli-or-background-agents">Accelerate using CLI or background agents</h3> <p>I will say that tools like <a href="http://www.ampcode.com">Amp</a>are amazing to achieve terminal velocity in adding new features once you’ve got the design pattern down. Not entirely sure why that is, but, I think it has to do with the efficient use of the context window and the ability to run command line tools and git with far better ease than in IDE. What I particularly like about Amp is that I can paste the contents of the conversation by sharing it.</p> <h2 id="what-excites-me-moving-forward">What excites me moving forward?</h2> <h3 id="personal-memory-and-memory-reasoning">Personal memory and memory reasoning</h3> <p>My most recent edit to Clairvoyant has been to add <a href="https://honcho.dev">Honcho</a>. Honcho is a dead simple method of way of embedding memory and reasoning into any application. It was one tool call, one handler, and one insertion into every existing handler to capture both the user’s vocal “diatribe” and a “synthesis” peer. I think of peers as the Agent’s shadow. They’re there to passively listen in on a conversation between you and the AI agent or the AI agents and other agents. Creating new peers is as simple as this:</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">export</span> <span class="k">async</span> <span class="kd">function</span> <span class="nf">initializeMemory</span><span class="p">():</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="p">[</span><span class="nx">Session</span><span class="p">,</span> <span class="nx">Peer</span><span class="p">[]]</span><span class="o">&gt;</span> <span class="p">{</span>
	<span class="kd">const</span> <span class="nx">honchoClient</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Honcho</span><span class="p">({</span>
		<span class="na">apiKey</span><span class="p">:</span> <span class="nx">env</span><span class="p">.</span><span class="nx">HONCHO_API_KEY</span><span class="p">,</span>
		<span class="na">environment</span><span class="p">:</span> <span class="dl">"</span><span class="s2">production</span><span class="dl">"</span><span class="p">,</span>
		<span class="na">workspaceId</span><span class="p">:</span> <span class="dl">"</span><span class="s2">with-context</span><span class="dl">"</span><span class="p">,</span>
	<span class="p">});</span>
	<span class="c1">// [1] Create a session</span>
	<span class="kd">const</span> <span class="nx">session</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">honchoClient</span><span class="p">.</span><span class="nf">session</span><span class="p">(</span><span class="nf">randomUUID</span><span class="p">());</span>
	<span class="c1">// [2] Create the peer(s)</span>
	<span class="kd">const</span> <span class="nx">diatribePeer</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">honchoClient</span><span class="p">.</span><span class="nf">peer</span><span class="p">(</span><span class="dl">"</span><span class="s2">diatribe</span><span class="dl">"</span><span class="p">,</span> <span class="p">{</span>
		<span class="na">metadata</span><span class="p">:</span> <span class="p">{</span>
			<span class="na">name</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Diatribe</span><span class="dl">"</span><span class="p">,</span>
			<span class="na">description</span><span class="p">:</span>
				<span class="dl">"</span><span class="s2">A peer that listens to the raw translations of the users' speech.</span><span class="dl">"</span><span class="p">,</span>
		<span class="p">},</span>
	<span class="p">});</span>
	<span class="kd">const</span> <span class="nx">synthesisedPeer</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">honchoClient</span><span class="p">.</span><span class="nf">peer</span><span class="p">(</span><span class="dl">"</span><span class="s2">synthesis</span><span class="dl">"</span><span class="p">,</span> <span class="p">{</span>
		<span class="na">metadata</span><span class="p">:</span> <span class="p">{</span>
			<span class="na">name</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Synthesis Peer</span><span class="dl">"</span><span class="p">,</span>
			<span class="na">description</span><span class="p">:</span>
				<span class="dl">"</span><span class="s2">A peer that captures synthesiszed  knowledge from the user's speech.</span><span class="dl">"</span><span class="p">,</span>
		<span class="p">},</span>
	<span class="p">});</span>
	<span class="c1">// Add the peers to the session</span>
	<span class="k">await</span> <span class="nx">session</span><span class="p">.</span><span class="nf">addPeers</span><span class="p">([</span><span class="nx">diatribePeer</span><span class="p">,</span> <span class="nx">synthesisedPeer</span><span class="p">]);</span>
	<span class="k">return</span> <span class="p">[</span><span class="nx">session</span><span class="p">,</span> <span class="p">[</span><span class="nx">diatribePeer</span><span class="p">,</span> <span class="nx">synthesisedPeer</span><span class="p">]];</span>
<span class="p">}</span>
</code></pre></div></div> <p>Instantiating the session happens (in my case) at every time the user turns on the Clairvoyant application in MentraOS. The memory session is then passed to every handler and every tool that needs it. God I love typescript so much!</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	<span class="k">protected</span> <span class="nx">override</span> <span class="k">async</span> <span class="nf">onSession</span><span class="p">(</span><span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">):</span> <span class="nb">Promise</span><span class="o">&lt;</span><span class="k">void</span><span class="o">&gt;</span> <span class="p">{</span>
		<span class="kd">const</span> <span class="p">[</span><span class="nx">memorySession</span><span class="p">,</span> <span class="nx">peers</span><span class="p">]</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">initializeMemory</span><span class="p">();</span>
		<span class="c1">// Other code</span>
		<span class="p">...</span>
	<span class="p">}</span>
</code></pre></div></div> <p>Now, when the individual asks a question (say, “What’s my name”) the LLM router will intelligently route this back to the <code class="language-plaintext highlighter-rouge">MEMORY_RECALL</code> handler and then call a memory recall command, which, under the hood is just a wrapper for the API endpoint that Honcho has created.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">export</span> <span class="k">async</span> <span class="kd">function</span> <span class="nf">MemoryRecall</span><span class="p">(</span>
	<span class="nx">textQuery</span><span class="p">:</span> <span class="kr">string</span><span class="p">,</span>
	<span class="nx">session</span><span class="p">:</span> <span class="nx">AppSession</span><span class="p">,</span>
	<span class="nx">memorySession</span><span class="p">:</span> <span class="nx">Session</span><span class="p">,</span>
	<span class="nx">peers</span><span class="p">:</span> <span class="nx">Peer</span><span class="p">[],</span>
<span class="p">)</span> <span class="p">{</span>
	<span class="kd">const</span> <span class="nx">runId</span> <span class="o">=</span> <span class="nb">Date</span><span class="p">.</span><span class="nf">now</span><span class="p">();</span>
	<span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">set</span><span class="p">(</span><span class="nx">session</span><span class="p">,</span> <span class="nx">runId</span><span class="p">);</span>

	<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span><span class="s2">`[startMemoryRecallFlow] Starting memory recall flow`</span><span class="p">);</span>

	<span class="k">try</span> <span class="p">{</span>
		<span class="kd">const</span> <span class="nx">diatribePeer</span> <span class="o">=</span> <span class="nx">peers</span><span class="p">.</span><span class="nf">find</span><span class="p">((</span><span class="nx">peer</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nx">peer</span><span class="p">.</span><span class="nx">id</span> <span class="o">===</span> <span class="dl">"</span><span class="s2">diatribe</span><span class="dl">"</span><span class="p">);</span>
		<span class="k">if </span><span class="p">(</span><span class="nx">diatribePeer</span><span class="p">)</span> <span class="p">{</span>
			<span class="c1">// [1] Capture the query as a memory first</span>
			<span class="k">await</span> <span class="nx">memorySession</span><span class="p">.</span><span class="nf">addMessages</span><span class="p">([</span>
				<span class="p">{</span>
					<span class="na">peer_id</span><span class="p">:</span> <span class="nx">diatribePeer</span><span class="p">.</span><span class="nx">id</span><span class="p">,</span>
					<span class="na">content</span><span class="p">:</span> <span class="nx">textQuery</span><span class="p">,</span>
					<span class="na">metadata</span><span class="p">:</span> <span class="p">{</span>
						<span class="na">timestamp</span><span class="p">:</span> <span class="k">new</span> <span class="nc">Date</span><span class="p">().</span><span class="nf">toISOString</span><span class="p">(),</span>
						<span class="na">source</span><span class="p">:</span> <span class="dl">"</span><span class="s2">memoryRecall</span><span class="dl">"</span><span class="p">,</span>
					<span class="p">},</span>
				<span class="p">},</span>
			<span class="p">]);</span>

			<span class="c1">// [2] Query Honcho's representation of your memory from a peer. </span>
			<span class="kd">const</span> <span class="nx">response</span> <span class="o">=</span> <span class="k">await</span> <span class="nf">showTextDuringOperation</span><span class="p">(</span>
				<span class="nx">session</span><span class="p">,</span>
				<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Trying to remember...</span><span class="dl">"</span><span class="p">,</span>
				<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Got it!</span><span class="dl">"</span><span class="p">,</span>
				<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Couldn't remember!</span><span class="dl">"</span><span class="p">,</span>
				<span class="p">()</span> <span class="o">=&gt;</span> <span class="nx">diatribePeer</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="nx">textQuery</span><span class="p">),</span>
			<span class="p">);</span>
			<span class="k">if </span><span class="p">(</span><span class="nx">response</span><span class="p">)</span> <span class="p">{</span>
				<span class="k">if </span><span class="p">(</span><span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
					<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
						<span class="s2">`[startMemoryRecallFlow] Response arrived for stale request, discarding`</span><span class="p">,</span>
					<span class="p">);</span>
					<span class="k">return</span><span class="p">;</span>
				<span class="p">}</span>
				
				<span class="c1">// [3] Format that memory response for the glasses. </span>
				<span class="kd">const</span> <span class="nx">memoryRecall</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">b</span><span class="p">.</span><span class="nc">MemoryQueryRecall</span><span class="p">(</span><span class="nx">textQuery</span><span class="p">,</span> <span class="nx">response</span><span class="p">);</span>

				<span class="k">if </span><span class="p">(</span><span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
					<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
						<span class="s2">`[startMemoryRecallFlow] Response arrived for stale request, discarding`</span><span class="p">,</span>
					<span class="p">);</span>
					<span class="k">return</span><span class="p">;</span>
				<span class="p">}</span>

				<span class="c1">// [4] Process the memory recall results for the glasses</span>
				<span class="k">if </span><span class="p">(</span>
					<span class="nx">memoryRecall</span><span class="p">.</span><span class="nx">results</span><span class="p">?.</span><span class="nx">lines</span> <span class="o">&amp;&amp;</span>
					<span class="nx">memoryRecall</span><span class="p">.</span><span class="nx">results</span><span class="p">.</span><span class="nx">lines</span><span class="p">.</span><span class="nx">length</span> <span class="o">&gt;</span> <span class="mi">0</span>
				<span class="p">)</span> <span class="p">{</span>
					<span class="kd">const</span> <span class="nx">lines</span> <span class="o">=</span> <span class="nx">memoryRecall</span><span class="p">.</span><span class="nx">results</span><span class="p">.</span><span class="nx">lines</span><span class="p">;</span>

					<span class="c1">// Display each line sequentially</span>
					<span class="k">for </span><span class="p">(</span><span class="kd">let</span> <span class="nx">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">lines</span><span class="p">.</span><span class="nx">length</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
						<span class="kd">const</span> <span class="nx">line</span> <span class="o">=</span> <span class="nx">lines</span><span class="p">[</span><span class="nx">i</span><span class="p">];</span>


						<span class="k">if </span><span class="p">(</span><span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">!==</span> <span class="nx">runId</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>

						<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">info</span><span class="p">(</span>
							<span class="s2">`[startMemoryRecallFlow] Memory recall line: </span><span class="p">${</span><span class="nx">line</span><span class="p">}</span><span class="s2">`</span><span class="p">,</span>
						<span class="p">);</span>
						<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="s2">`// Clairvoyant\nR: </span><span class="p">${</span><span class="nx">line</span><span class="p">}</span><span class="s2">`</span><span class="p">,</span> <span class="p">{</span>
							<span class="na">view</span><span class="p">:</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span>
							<span class="na">durationMs</span><span class="p">:</span> <span class="mi">3000</span><span class="p">,</span>
						<span class="p">});</span>

						<span class="c1">// Add delay between lines (except for the last line)</span>
						<span class="k">if </span><span class="p">(</span><span class="nx">i</span> <span class="o">&lt;</span> <span class="nx">lines</span><span class="p">.</span><span class="nx">length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
							<span class="k">await</span> <span class="k">new</span> <span class="nc">Promise</span><span class="p">((</span><span class="nx">resolve</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nf">setTimeout</span><span class="p">(</span><span class="nx">resolve</span><span class="p">,</span> <span class="mi">3000</span><span class="p">));</span>
						<span class="p">}</span>
					<span class="p">}</span>
				<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
					<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span>
						<span class="s2">`[startMemoryRecallFlow] No lines in memory recall results`</span><span class="p">,</span>
					<span class="p">);</span>
					<span class="k">if </span><span class="p">(</span><span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">===</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
						<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span>
							<span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: No memories found.</span><span class="dl">"</span><span class="p">,</span>
							<span class="p">{</span>
								<span class="na">view</span><span class="p">:</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span>
								<span class="na">durationMs</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
							<span class="p">},</span>
						<span class="p">);</span>
					<span class="p">}</span>
				<span class="p">}</span>
			<span class="p">}</span>
		<span class="p">}</span>
	<span class="p">}</span> <span class="k">catch </span><span class="p">(</span><span class="nx">error</span><span class="p">)</span> <span class="p">{</span>
		<span class="nx">session</span><span class="p">.</span><span class="nx">logger</span><span class="p">.</span><span class="nf">error</span><span class="p">(</span>
			<span class="s2">`[startMemoryRecallFlow] Error recalling memory: </span><span class="p">${</span><span class="nx">error</span><span class="p">}</span><span class="s2">`</span><span class="p">,</span>
		<span class="p">);</span>
		<span class="k">if </span><span class="p">(</span><span class="nx">memoryRunCallIds</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="nx">session</span><span class="p">)</span> <span class="o">===</span> <span class="nx">runId</span><span class="p">)</span> <span class="p">{</span>
			<span class="nx">session</span><span class="p">.</span><span class="nx">layouts</span><span class="p">.</span><span class="nf">showTextWall</span><span class="p">(</span><span class="dl">"</span><span class="s2">// Clairvoyant</span><span class="se">\n</span><span class="s2">R: Couldn't remember!</span><span class="dl">"</span><span class="p">,</span> <span class="p">{</span>
				<span class="na">view</span><span class="p">:</span> <span class="nx">ViewType</span><span class="p">.</span><span class="nx">MAIN</span><span class="p">,</span>
				<span class="na">durationMs</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
			<span class="p">});</span>
		<span class="p">}</span>
	<span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>What’s really impressive is that in the default route where no tool is called, almost all the conversation that I’m having is being logged into memory. I now understand why tools like <a href="friend.com">friend</a>, <a href="https://tayanecklace.com">taya</a>and i’m sure a litany of others all have some form of appeal. Its simple, being able to recall with perfect clarity what you said (or what others have said) is of high value to people. I know I’d like to win every argument with my wife!</p> <h3 id="devices-with-beam-forming-microphones">Devices with beam forming microphones</h3> <p>I’m not an audio specialist but I do have enough of a physics understanding to appreciate the need to perform the necessary signal processing to amplify the “target”’s voice and separate it from the noise. In this sense, the G1’s are not a sufficiently good device. I’m hopeful, however, that as the hardware progresses there will be the ability to add multiple microphones and then perform the signal processing on device to clear up the speaker’s intended sound. Sometimes when I want to be really crisp in what I’m recording, I’ll just wear my Airpods and my glasses and the voice capture and transcription becomes instantly better.</p> <h3 id="improving-latency-in-ai-models">Improving latency in AI models</h3> <p>I think we’re finally out of the trough of llusion for transformer architecture based models. Owing to vLLM’s coming through, we’re no longer sitting around for minutes waiting for the LLM to give us back an answer. Soon, this will be fast, in under 1s, and feel snappy. I am hopeful that there will be more attention to this domain in 2026.</p> <h3 id="diarization">Diarization</h3> <p>Even if we could just have self versus other in a latency sensitive way, that would be pretty cool. I’m thinking through how to support diarization. Something like an onboarding to capture the vocal data and have that as a check before sending data to a VAD model would be kind of neat. Of course, the ecosystem in Mentra and Soniox would have to be better resolved to support this.</p> <h3 id="shared-human-memory">Shared human memory</h3> <p>This work has got me thinking about memory at a deeper level. What if AI glasses become democratized? If everyone had memory embedded into their glasses, then could we all just have access to each other’s memory? Could we create a “hivemind” type experience where AI becomes the propagator of shared intuition, reasoning, and understanding? More on this when the time is right!</p> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/10/061f8410b0ae1ad37c390f5cec46881e.png" alt="image"/></p> <h2 id="conclusion">Conclusion</h2> <p>Clairvoyant started with a curiosity about an old word and turned into a real system for clear-seeing. It takes audio, turns it into text, routes that text through agents, and pushes context back into the HUD. The hard part hasn’t been the models or the hardware. It has been about speed, structure, and design choices that make the experience feel natural. Latency, memory, and routing are not abstract problems here, they are the core of making the assistant actually useful. If ancient clairvoyance was about touching something beyond human reach, the modern version is about capturing what we said, what we heard, and what we want to remember. The real work is in writing opinionated code, testing agents, separating tools from handlers, and making patterns that other developers can build on. It is time to lock in and build the software foundations now so that when the glasses get better and the models get faster, the system already feels like second nature.</p>]]></content><author><name></name></author><category term="machine-learning-posts"/><category term="ar"/><category term="ai"/><category term="agents"/><category term="clairvoyant"/><category term="voice"/><category term="transcription"/><category term="memory"/><summary type="html"><![CDATA[Building Clairvoyant - a real-time voice transcription and intelligent context-aware assistant for AR glasses that captures audio, processes it through AI agents, and provides personalized responses.]]></summary></entry><entry><title type="html">Uniting the digital and visual world With Context</title><link href="https://ajay-bhargava.github.io/blog/2025/uniting-visual-world/" rel="alternate" type="text/html" title="Uniting the digital and visual world With Context"/><published>2025-07-04T16:00:00+00:00</published><updated>2025-07-04T16:00:00+00:00</updated><id>https://ajay-bhargava.github.io/blog/2025/uniting-visual-world</id><content type="html" xml:base="https://ajay-bhargava.github.io/blog/2025/uniting-visual-world/"><![CDATA[<h2 id="foreword-into-augmented-reality-ar">Foreword into Augmented Reality (AR)</h2> <p>There are two equally rich worlds. The real world we move through (the visual world), and the digital world of the internet, books, data servers, and LLM’s that live stationary at fixed entry points such as laptops, phones, and increasingly on audio interface touch points.</p> <p>Our brains are not designed to unite the vast visual world with the equally vast digital world. As a consequence of this fact, I believe that AR embedded with AI empowered assisted decision support software systems that bridge these two worlds represents the same gain of function for society to the extent ChatGPT empowered humanity to conquer the digital world.</p> <h2 id="what-do-the-brains-biological-limitations-tell-us">What do the brain’s biological limitations tell us?</h2> <p>From the retinal photoreceptors to our occipital lobes, a daisy-chain of hierarchically organized neurons of many kinds achieve 1000:1 compression of our visual world into concise information that can be reasoned with by our brains in real time. The net effect of this is that you never think about 99.9% of what your eyes see because an entire visual executive “assistive” repertoire is “handling it” before the rest of your brain needs to care about it.</p> <p>Reading can, by comparison, feel efficient when content is compressed. Yet, it becomes inefficient as character count you read grows in size. This is because text is symbolic and linear. Yes, syntactically, language is compressed, but requires sequential decoding to make sense of it even when focused solely on reading. This fundamental difference becomes even more clear when what you read earlier has to be paired with live visual input you’re seeing now.</p> <p>Why do we suck at this task? It’s our biology and an evolutionary legacy! Separate neural subsystems are engaged between text (Broca’s, Wernike’s areas) and vision (V1-V4, IT). These systems are operating in parallel. Integration between these two neural subsystems only happens in higher order neural structures such as your prefrontal cortex and that structure is slow and capacity limited. There’s also little pre-computation of text compared to visual information. The hierarchy of daisy-chained visual neurons enables pre-compilation of visual information. There is no such hierarchy for language based information. From a biological standpoint it justifies why tools like GPT that summarize, diagram etc., have taken off in such a dramatic way: these models offload and “assist” the job that your pre-frontal cortex used to spend manual cycles on onto a external digital-organ that pre-processes this for you.</p> <p>You’ve probably unknowingly got an intuition of how humanity has mitigated for our evolutionary bottleneck and why it gives us delight when pre-computation of the visual and digital world is done for you (and makes others money). If you’ve looked at an ad or just about any marketing you’ll know that great marketing is a combination of highly compressed text and visual cues that’s insightful, actionable, and persuasive. Someone has done the visual and textual pre-computation to make the process of making (or swaying) a decision for you trivially easy.</p> <blockquote class="callout callout-important"> <p>Anywhere money is made through rapid perception and low-effort cognition you will find systems that pre-compute visual-text mappings.</p> </blockquote> <p>First, computer vision models parallelized visual computation to beyond the parallelization limit of human abilities. Then, LLM’s became the de-facto extra-corporeal organ for textual pre-computation. What’s next? I think its application of AI that uses multi-modal input prompts that is then in turn used to unlock and gather from the vast digital world by commanding networks of AI agents. Returned information has to be thoughtfully compressed using AI at the interface that people would derive the most delight from being able to act upon this information.</p> <p>Finding the right visual cues to unlock the right access of the relevant digital information in a way thats thoughtful and actionable is the core concept of a decision intelligence SDK and tooling layer that I want to build for the world.</p> <h2 id="what-can-we-learn-from-the-army">What can we learn from the Army?</h2> <h3 id="thank-you-to-our-military-service-members">Thank you to our military-service members</h3> <p>I spent months working intensively on projects with AFC (now TRADOC) when I was at Actuate and that culminated in a test of my work in a life-fire exercise on a base (poorly pictured below - lets just say an M109A7 firing down range will scare the shit out of you). I might look like a total dork here but it was honestly one of the most humbling and informative learning experiences of my life. I am truly humbled by our service-members. Without this there would be no concept of “With Context”.</p> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/07/4ad64a26ce60435fd5fa496797c16bf8.jpg" alt="image" width="100%"/></p> <h3 id="using-well-designed-decision-support-ar-systems">Using well designed decision support AR systems</h3> <p>The U.S. Army and other military branches have decades of R&amp;D into heads-up-display (HUD) systems. You’ve probably seen them mounted on guns, mounted on pilots and lately in autonomous tracking. There are many design philosophies in cognitive ergonomics, information compression, and perceptual prioritization that help enforce I perceive how humans <em>should</em> experience AI that’s embedded in decision support interfaces such as AR glasses.</p> <h4 id="1-cognitive-bandwidth-is-a-ux-constraint">1. Cognitive bandwidth is a UX constraint</h4> <p><em>Too much data ≠ better decisions</em></p> <p>ChatGPT can’t help but spit out just gobs of text. Its insanity. The training runs responsible for aligned AI systems contribute to this problem. So much effort is put into libraries that constrain or structure these generations. The military has a clear doctrine on this:</p> <h4 id="2-data-relevance-beats-quality">2. Data relevance beats quality</h4> <p><em>Offer useful data, don’t offer useless data.</em></p> <p>Intriguingly, early HUD’s were poorly adopted because they dumped raw telemetry (altitude, speed, bearing) at all times in all contexts. Ever sit on an airplane today? Yeah, what good is knowing the distance to your destination when you’re landed at your destination? Or even the ground speed? Pointless.</p> <h4 id="3-intuitive-symbols-leverage-your-visual-cortex">3. Intuitive symbols leverage your visual cortex</h4> <p><em>Sprites and emojis are your friends</em></p> <p>Your mind is adapted to visual context and visual memory! Use it. Taking advantage of standardized symbols (think: Stop Sign, Warning, Road Closed) that are engrained in our visual memory is key to making the decisions actionable.</p> <h4 id="4-make-the-ai-output-aligned-with-the-visual-scene">4. Make the AI output aligned with the visual scene</h4> <p><em>Use vLLM’s to obfuscate prompting/programming</em></p> <p>One of the fastest ways to have HUD be unusable is to have them deliver context that isn’t relevant in that moment, that isn’t ephemeral and is high latency. These were impossible barriers to cross not more than 1 year ago as vLLM’s were still in their infancy. Fortunately, companies like <a href="http://moondream.ai">Moondream</a>are tackling this problem head on. Tiny LLM’s are the rage now, Tiny vLLM’s are the next.</p> <h4 id="5-leverage-attention-to-be-relevant-but-not-annoying">5. Leverage attention to be relevant but not annoying</h4> <p><em>Don’t be annoying</em></p> <p>Ok this one isn’t from the military but brought up from a Bronx native - yes, <a href="https://youtu.be/AWN8VmCTXAU?si=vUBhRzLY2wdDk2M-">Chris Hayes</a>. In an interview with Hasan Minhaj he talks about his book about human attention. Specifically, there are two kinds of attention: voluntary, and compelled. We are subjected to compelled attention mechanisms (e.g. car is honking, should I respond? Phone is buzzing, should I respond?). In things like AR glasses we have to be mindful of the fact that the wearer likely has one other device in his pocket begging for his attention. The Apple Watch has done a better job of being less attention seeking and therefore more relevant (I think). Similarly for UX design we have to think carefully about how the interface is to wield your attention and then unwield it just as fast so as to prevent fatigue, feel ambient, and feel ephemeral. This ephemeral, context driven nature of the equation is also key for a business model here.</p> <h4 id="summary">Summary</h4> <p>A friend and collaborator once told me that wearable AR tech for decision-making should feel like the difference between a good executive assistant (EA) and a bad one. I’ve never had an EA myself, but here’s how I imagine it. If you’re heading up an elevator to a big boardroom meeting, the good EA is prepping you on the way up. They’re making sure the critical context is top of mind right before you walk in. That’s how good military HUD systems work. A solid AR decision support system ensures you’re not just present, but prepared. It makes you capable in ways that go beyond what you could manage alone.</p> <p>The hackathon where we built “EyyyyWear” was our way of pressure-testing this idea. It forced us to build AI agent overlays under real constraints. As developers, we felt those constraints in code. Delivering decision support while humans are in motion is not easy. But that’s where the value shows up. You check if a parking spot is open while your car is still moving. Why? So you can stay nimble in your car, avoid wasting time, and skip a parking ticket all while keeping your hands on the wheel.</p> <p>The hackathon at Betaworks was just an example. The larger point is that these systems are most useful when they meet you in real time, in your flow, in-context. The word “context” really matters here because its a sharp break from the way smartphone apps were designed. Lets get into that next.</p> <h2 id="how-will-ar-powered-with-context-be-more-delightful-to-us-than-smartphones">How will AR powered <code class="language-plaintext highlighter-rouge">With Context</code> be more delightful to us than smartphones?</h2> <p>In the era of smartphones: users tap icons, add text, and scroll through apps bought on app stores. Your world is inside the app. In the wearable AR world, I believe that real world is your interface. What appears in your AR glasses while you’re in the real world is driven by the contexts you subscribe to. <code class="language-plaintext highlighter-rouge">Contexts</code> are overlaid AI agents that use images as sophisticated prompts to other models and leverage other AI agents to provision digital datasets to bring meaning to your visual world. I’d love to use this next section to make a case for the fundamental shift between apps and contexts.</p> <h3 id="they-will-provide-context-not-experiences">They will provide <code class="language-plaintext highlighter-rouge">Context</code>, not “Experiences”</h3> <p>The smartphone app is an “intention-driven” interaction. You buy it, you touch its interface, you let the app decide, you share your experience on it with others. Whether you’re present in our earthly environment or not is beside the point when using an app. You’ve probably seen this when you’re with your friends and they go on their phones. When they’re on the app, they’re <em>in</em> the app. The app is running in their mental foreground, and the developers who created the app have curated the runtime “environment” to keep you engaged. Monetization in the app model is dependent on how much more of the app you want to lock into. Humanity has adapted, and subsequently become fatigued, to the concept and economics of the “app”.</p> <p>In AR, because the visual world is the interface, we’ll be accessing <strong>different</strong> overlay AI agents that own and curate relevant context to your interface at that moment. I call them <code class="language-plaintext highlighter-rouge">Contexts</code> for short. They wont be something you touch, or intentionally download, they will function specific to visual context. You will be the decision engine, not the app. The net effect of this is: you won’t have to unravel structured visual understanding into low bitrate “words” with your brain, <code class="language-plaintext highlighter-rouge">Contexts</code> will see what you see and do what you normally would’ve done with your phone to aid your decision making. I believe this will be of higher use to society than apps ever were.</p> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/07/8e7e3e3df2f285fb45ede004c3c60428.png" alt="image" width="100%"/></p> <h3 id="monetization-is-ambient-and-quality-reinforcing">Monetization is ambient and quality reinforcing</h3> <p>With smartphone apps, you are paying money detach yourself from the visual world. You then pay even more money for more expensive applications that promise that with AI and AI agents, your time on the app and therefore the length of time you’re detached from the world will shorten.</p> <p>I think the monetization strategy of “eyeball commitment” is one ripe to be flipped on its head. It goes without saying that desktop class applications like “Cluely” which bring decision intelligence as a service to the desktop have scratched a societal itch on this exact business model. To me, this provides validation that we’re touching a very real sore-spot that’s driven by our biology. Cluely’s business model, however, is still very SaaS centric ($20/month) because it lives atop other SaaS applications.</p> <p>I believe we should monetize those AI’s that best function in the digital world where it “sees best”, and shepherd that returned information back to the physical one where we “see” best. Instead of paying to unlock an app, you pay to unlock the right “Contexts” for you.</p> <table> <thead> <tr> <th>Apps</th> <th>Contexts</th> </tr> </thead> <tbody> <tr> <td>App Purchase</td> <td>No fee to download the “Context Platform”</td> </tr> <tr> <td>App Store fees</td> <td>Usage fee for different Contexts</td> </tr> <tr> <td>In-app purchases</td> <td>Higher usage fee for more involved, deeper knowledge parsing or more computationally complex AI agents that require longer reasoning</td> </tr> </tbody> </table> <hr/> <h3 id="deliver-actionable-insight-not-just-delight">Deliver actionable insight, not just delight</h3> <p>Most apps don’t leave you with an “actionable” insight, despite leaving you delighted by the UX, UI or a combination of the two. Owing to the constrained interfaces of a heads up display on a pair of glasses, outputs need to be short, punchy, and to the point. Designing for this constraint has to exist at the core cultural level of any “Context” creator. With Context will provide the MCP adaptors, tooling, and even other models in the loop to ensure that the user experience is actionable and purposeful.</p> <h2 id="existing-participants-that-lower-the-barrier">Existing participants that lower the barrier</h2> <h3 id="software-infrastructure">Software Infrastructure</h3> <p>Every AR device manufacturer has some form of a developer SDK that enables device level control of the hardware. <a href="https://github.com/Mentra-Community/MentraOS?tab=readme-ov-file">Mentra OS</a> is no exception here, providing abstraction over multiple devices to enable developers to readily create applications for a suite of AR glasses.</p> <p><img src="https://with-context-public.s3.us-east-1.amazonaws.com/internal-memory-documents/2025/07/50c006594f9e6c81ebe88c5d66fd2a48.png" alt="image" width="100%"/></p> <h4 id="note-about-mentra">Note about Mentra</h4> <p>MentraOS and Mentra Merge are upcoming products alongside Mentra’s glasses. They are also cognizant about AI agents and I believe are well suited to produce the sort of AI overlay context agents I am talking about.</p> <h3 id="vllm-models">vLLM models</h3> <p>There are an increasing number of vLLM makers, most notably <a href="https://moondream.ai">Moondream.ai</a>who are uniquely solving both minification of vLLM’s and also increasing model throughput and inference speed. These represent important bridges between the visual and digital world.</p> <h3 id="digital-ai-agent-makers">Digital AI agent makers</h3> <p>It goes without saying that this venture cannot exist without an ever growing army of AI agents and MCP servers making their way into production grade applications. Even just 1 year ago when some of these AR devices were manufactured and sold for the first time, the concept of AI agents did not exist. Deeper and potentially interesting enterprise grade integrations using digital AI agents will fuel interesting use cases for AR “Contexts”.</p> <h2 id="development-plan">Development Plan</h2> <table> <thead> <tr> <th style="text-align: center">Level</th> <th>Focus</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">1</td> <td>Validate a Device SDK</td> <td>Despite the fact that there are hardware SDK providers, not all devices are on the same SDK framework (e.g. Typescript). Fortunately most do have the underlying primitives and open documentation to get started.</td> </tr> <tr> <td style="text-align: center">2</td> <td>Prototype/Sell manually designed use case</td> <td>We previously demo’ed EyyyWear for a parking use case, and have eyes on other more monetizable use cases in sales and potentially sports applications to gauge business interest. We will leverage existing tooling and platform where possible to deploy quickly.</td> </tr> <tr> <td style="text-align: center">3</td> <td>Survey the landscape of AI Agents suitable for Context Platform</td> <td>Qualify sustained interest and pitch if other contexts (overlay AI agents) would be useful in a decision intelligence setting.</td> </tr> <tr> <td style="text-align: center">4</td> <td>Design Context Subscription Platform</td> <td>If sustained interest is yielded then we will move into designing a context subscription platform “app” that will serve as the “app platform” or “store” for multiple contexts.</td> </tr> <tr> <td style="text-align: center">5</td> <td>Create adaptors for existing MCP’s or SDK for AI agents (or create new MCP’s)</td> <td>Some business interesting AI agents might not be suitable for decision intelligence, they will need to be modified or adapted to functionally return actionable insights or intelligence.</td> </tr> <tr> <td style="text-align: center">6</td> <td>Automate decision intelligence with novel AI models and evaluate existing vLLM models</td> <td>We will need to better prompt or fine-tune an AI to tailor responses to be actionable for decision support given diverse outputs from multiple Contexts and multiple visual world inputs.</td> </tr> <tr> <td style="text-align: center">8</td> <td>Evaluate AR hardware performance for future use cases</td> <td>We may need to progress to developing our own hardware to better suit AI / AR needs.</td> </tr> </tbody> </table> <hr/> <h2 id="further-business-opportunities">Further business opportunities</h2> <p>Here are some example uses that one could use <code class="language-plaintext highlighter-rouge">With Context</code>’s <code class="language-plaintext highlighter-rouge">Context</code> agents in:</p> <table> <thead> <tr> <th><strong>Trigger</strong></th> <th><strong>Overlay Response</strong></th> <th>Context Agent Complexity</th> </tr> </thead> <tbody> <tr> <td>👀 Look at a restaurant</td> <td>Show Yelp score, menu, wait time</td> <td>Low</td> </tr> <tr> <td>🤲 Hold a prescription bottle</td> <td>Show dosage instructions, and number of refills available.</td> <td>Low</td> </tr> <tr> <td>🔧 Face HVAC equipment</td> <td>Show when the next filter change ought to happen.</td> <td>Medium</td> </tr> <tr> <td>📸 Gaze at a person at a sales event</td> <td>Show lead history, next action, notes.</td> <td>High</td> </tr> </tbody> </table> <hr/> <p>There really are a number of use-cases that could also be worked on with branded partnerships such as those with sports leagues for entertainment or enhanced insights for fans who would appreciate that level of engagement.</p> <h1 id="summary-1">Summary</h1> <p>Augmented reality (AR) and artificial intelligence (AI) together offer a powerful method to bridge the visual and digital worlds, addressing our human cognitive limit. Our brains naturally separate visual and textual information into different processing pathways, leading to inefficiencies when integrating both simultaneously. A fundamental shift in monetization from traditional app-driven models to context-driven overlays can enhance user experience significantly. Fortunately, the existing ecosystem already includes robust infrastructure such as device SDKs, streamlined visual-language models (vLLMs), and advanced digital AI agents, providing a solid foundation for developing <code class="language-plaintext highlighter-rouge">Contexts</code>. If actionable intelligence <code class="language-plaintext highlighter-rouge">With Context</code> sounds interesting to you, reach out to me! I am building in this space.</p>]]></content><author><name></name></author><category term="machine-learning-posts"/><category term="ar"/><category term="ai"/><category term="agents"/><category term="context"/><summary type="html"><![CDATA[A call to action to unite the digital and visual world with AI agents and AR glasses.]]></summary></entry><entry><title type="html">Tracking the precise dropoff location of a delivery in your building.</title><link href="https://ajay-bhargava.github.io/blog/2025/timeseries-machine-learning/" rel="alternate" type="text/html" title="Tracking the precise dropoff location of a delivery in your building."/><published>2025-06-16T16:00:00+00:00</published><updated>2025-06-16T16:00:00+00:00</updated><id>https://ajay-bhargava.github.io/blog/2025/timeseries-machine-learning</id><content type="html" xml:base="https://ajay-bhargava.github.io/blog/2025/timeseries-machine-learning/"><![CDATA[<h1 id="tldr">Tl;DR</h1> <p>I built a time-series classification model that could classify whether or not a person was indoors or outdoors based on the data collected by the 9-DoF sensor on their phone because I was excited to apply to a senior machine learning engineer role at <a href="http://www.doorstep.ai">Doorstep.ai</a>. Unfortunately, the company emphasized Leetcode and I did not pass the coding round because i couldn’t code up a solution to a palindrome substring problem. I wrote this project in the span of 3 days over the weekend. The deployable model and steps to replicate the training of the model are <a href="https://github.com/ajay-bhargava/extrasensory">here</a>.</p> <h1 id="you-can-just-do-things-with-time-series-data">You can just do things with time-series data</h1> <p>It is no secret that I love time-series datastreams and I think humanity should get really good at reading, understanding, leveraging, and making predictions from time-series data.</p> <p>I’ve gleaned some insights while using time-series datasets over the years. Here’s a few:</p> <h2 id="learn-how-to-sleep-better">Learn how to sleep better</h2> <p>When I worked on a project with <a href="https://www.fulcradynamics.com/magazine/are-your-devices-trying-to-tell-you-something-unlocking-personal-health-insights-through-data-with-data-scientist-ajay-bhargava">Fulcra Dynamics</a> I leveraged HealthKit data and other raw data from a continuous glucose monitor. I assembled data at a per second interval and cleansed over 6 months of collection data to produce a multi-modal machine learning approach to find that a patient could get one extra hour of sleep if they just ate their last meal 2-3 hours earlier than they normally did. Intriguingly this is very consistent with the work of <a href="https://en.wikipedia.org/wiki/Jürgen_Aschoff">Jürgen Aschoff</a> who found that the body’s circadian rhythm is regulated by multiple cues, including the timing of meals.</p> <p>This dataset was incredibly rich and clean from biased actions because the data was passively collected and required no human intervention. Sure, you could knock the collection accuracy and the precision of the data relative to the human event but that’s an improving trend over time as electronics become more sensitive and the data collection becomes more precise.</p> <h2 id="have-a-job-that-cant-be-stolen-by-ai">Have a job that can’t be stolen by AI</h2> <p>Its no secret that time-series models are incredibly dense matricies. In a typical time-series tensor you’ve got three dimensions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">tensor</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">],</span>
    <span class="p">[</span>
        <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
    <span class="p">]</span>
<span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># &gt; (2, 2, 3)
</span>
</code></pre></div></div> <p>Where the first dimension is the sample you’re looking at and the second and third dimension could be the number of time-stamps collected and the number of features collected at each time-stamp. Consider that for each feature you may have a <code class="language-plaintext highlighter-rouge">np.float</code> at each dimensional position. If you were to place that entire tensor into the context window of a model, you’d very easily overwhelm the model’s window to accept your request. In fact, this is exactly why Google and others developed vision langugage models (vLLM’s) to handle dealing with time-series data in a way that’s more information efficient. I discuss this here:</p> <div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet" align="center"><p lang="en" dir="ltr">Remember kids: time-series data analysis with GPT&#39;s isn&#39;t gonna happen with long context windows eating raw time-series, its gonna happen with vLLM&#39;s understanding matplotlib plots: <a href="https://t.co/3F5CxVLZ7y">https://t.co/3F5CxVLZ7y</a></p>&mdash; AJ (@0x1F9ED) <a href="https://twitter.com/0x1F9ED/status/1886227564173111657?ref_src=twsrc%5Etfw">February 3, 2025</a></blockquote> <script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> </div> <h2 id="determine-slip-and-fall-events-from-camera-feeds">Determine slip and fall events from camera feeds</h2> <p>I’ve devised approaches to determine slip and fall events from camera feeds. Security cameras are a pecularity for traditional approaches to detect falling down or slipping. This is because the field of fiew and distance from camera is so variable. You end up getting a dataset of people who are either large, medium or tiny. Not being able to capture the focal points of a human body (e.g. face, shoulders, knees, etc.,) makes it hard to deploy traditional pose-estimation models which means you can’t detect falling down.</p> <p>Furthermore, another caveat with working with security cameras is that in order to emphasize detection speed, it is favorable to use YOLO models. These usually require bounding boxes of people as training data. I had a ton of this data on hand while working at Actuate. As a prototype project, I thought I could use a bounding-box to pixelwise estimation model to grok the instance mask of a person. Then, using this data I further trained up a YOLACT model to track the person’s instance mask over time. Using mask moments I was able to calculate the principle axis of the mask approximated over a time window then train up a LSTM to classify if a person was standing or falling.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/slip-and-fall-480.webp 480w,/assets/img/slip-and-fall-800.webp 800w,/assets/img/slip-and-fall-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/slip-and-fall.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="learn-why-cancer-grows-the-way-it-does">Learn why cancer grows the way it does</h2> <p>Lots of people today are pilled on either single-cell genomics or spatial transcriptomics. While these are either destructive or non-destructive ways to study cancer cells or cancer tissue <em>in-situ</em> they are not very powerful tools because cancer grows over space, and, crucially, <strong>time</strong>. If people do study cancer cells over time, they’re usually piecing a timeline of events from multiple populations of cancers across different patients and its usually met with mixed results each time.</p> <p>The holy grail is to study the same cancer over time from the initial cell all the way to the final size it grows against (whether that’s when the patient is dead or when the cancer hits a boundary condition). Fortunately, I devised a way to study this (forthcoming blog post). What I can say is that when I did study it this way, I found out <strong>why</strong> cancer tissue grows in a similar pattern every time, even though no two cells are identical.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/clonal-dynamics-480.webp 480w,/assets/img/clonal-dynamics-800.webp 800w,/assets/img/clonal-dynamics-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/clonal-dynamics.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To create time-series data from imaging data is incredibly challenging and yet incredibly powerful because you can glean how each individual cell behaves within a crowded space over time. By leveraging the right analysis methods for spatiotemporal datasets, you can essentially study matter in the way it actually exists in our world.</p> <h2 id="track-exactly-where-your-package-was-dropped-off">Track <em>exactly</em> where your package was dropped off</h2> <p>Ok, now on to the subject of this post. I’ve been fascinated by the value proposition of <a href="http://www.doorstep.ai">Doorstep.ai</a>. It appears from their website that they are a company that is interested in tracking the last 50ft of the delivery into your building. If you live in a major city and you don’t have a doorman in your building, you know exactly what this pain is.</p> <p>According to a <a href="https://chatgpt.com/share/685072fe-a21c-8012-97e4-c71c1e82b4bf">ChatGPT</a> driven business estimation via web-search, U.S. carriers absorb roughly $16 billion in write-offs every year when parcels disappear inside buildings or get mis-scanned at the door. A 9-DoF sensor fusion SDK that runs on a driver’s phone to generate a verifiable indoor drop-point coordinate could cut “lost in the last 50 ft” claims by about 30 percent and turning those savings into measurable ROI.</p> <h3 id="doorstepais-sdk-and-data-collection">Doorstep.ai’s SDK and data collection</h3> <p>Intriguingly they have a <a href="https://www.doorstep.ai/docs">SDK</a> that ostensibly runs on the delivery driver’s phone.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nx">React</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">react</span><span class="dl">'</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">DoorstepAIRoot</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">@doorstepai/dropoff-sdk</span><span class="dl">"</span><span class="p">;</span>
<span class="kd">const</span> <span class="nx">App</span> <span class="o">=</span> <span class="p">()</span> <span class="o">=&gt;</span> <span class="p">(</span>
  <span class="o">&lt;</span><span class="nx">DoorstepAIRoot</span> <span class="nx">apiKey</span><span class="o">=</span><span class="dl">"</span><span class="s2">YOUR_API_KEY</span><span class="dl">"</span><span class="o">&gt;</span>
<span class="p">);</span>
<span class="k">export</span> <span class="k">default</span> <span class="nx">App</span><span class="p">;</span>

<span class="c1">// Injection of collection data via SDK</span>
<span class="c1">// Start with Google PlaceID</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByPlaceID</span><span class="p">(</span><span class="dl">"</span><span class="s2">placeID_here</span><span class="dl">"</span><span class="p">);</span>

<span class="c1">// Start with Google PlusCode</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByPlusCode</span><span class="p">(</span><span class="dl">"</span><span class="s2">plusCode_here</span><span class="dl">"</span><span class="p">);</span>

<span class="c1">// Start with Address Components</span>
<span class="kd">const</span> <span class="nx">address</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">street_number</span><span class="p">:</span> <span class="dl">"</span><span class="s2">123</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">route</span><span class="p">:</span> <span class="dl">"</span><span class="s2">Main St</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">locality</span><span class="p">:</span> <span class="dl">"</span><span class="s2">City</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">administrative_area_level_1</span><span class="p">:</span> <span class="dl">"</span><span class="s2">State</span><span class="dl">"</span><span class="p">,</span>
    <span class="na">postal_code</span><span class="p">:</span> <span class="dl">"</span><span class="s2">12345</span><span class="dl">"</span>
<span class="p">};</span>
<span class="nx">DoorstepAI</span><span class="p">.</span><span class="nf">startDeliveryByAddressType</span><span class="p">(</span><span class="nx">address</span><span class="p">);</span>
</code></pre></div></div> <h3 id="9-dof-sensor-data">9-DoF sensor data</h3> <p>The basic premise of a 9-DoF sensor is that it can collect data from the following sensors:</p> <table> <thead> <tr> <th style="text-align: left">Sensor</th> <th style="text-align: center">Description</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Accelerometer</td> <td style="text-align: center">3-axis accelerometer</td> </tr> <tr> <td style="text-align: left">Gyroscope</td> <td style="text-align: center">3-axis gyroscope</td> </tr> <tr> <td style="text-align: left">Magnetometer</td> <td style="text-align: center">3-axis magnetometer</td> </tr> <tr> <td style="text-align: left">Light</td> <td style="text-align: center">Light sensor</td> </tr> <tr> <td style="text-align: left">Audio</td> <td style="text-align: center">Audio sensor</td> </tr> <tr> <td style="text-align: left">Barometer</td> <td style="text-align: center">Barometric pressure</td> </tr> <tr> <td style="text-align: left">GPS</td> <td style="text-align: center">Global Positioning System</td> </tr> </tbody> </table> <hr/> <p>These chips are on the phone’s System on a Chip (SoC) but conceptually they are measuring the movement of a “ball” inside a box with sensor endings. Graphically, it looks like this:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/9-dof-sensor-480.webp 480w,/assets/img/9-dof-sensor-800.webp 800w,/assets/img/9-dof-sensor-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/9-dof-sensor.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The 9-DoF sensor is a very powerful sensor that can be used to track the movement of a “ball” inside a box with sensor endings.</p> <blockquote> <p>I sought out to figure out how to build a ML model that could carry out useful classifications on the data collected by this SDK.</p> </blockquote> <p>Now, because I don’t have access to <a href="http://www.doorstep.ai">Doorstep.ai</a>’s data, I had to find my own. Below and in the rest of the blog post, I yap about my casual 2-day journey to build a model and inference server that could run on the data collected by this SDK. Ostensibly by the end you will learn how I have deployed an inference server that could be passed the data collected by the SDK and be used to fill out a dropoff report that a customer could access if they have trouble locating the parcel in their building.</p> <h1 id="finding-phone-sensor-data">Finding phone sensor data</h1> <p>There are many datasets available. I ended up going for the <strong>ExtraSensory</strong> dataset because it did not require too much data cleaning and preparation to get started, though, the SHL dataset is <em>very</em> appealing.</p> <table> <thead> <tr> <th>Dataset (year)</th> <th>Core modalities present</th> <th>Needs a “GPU”?</th> <th>Inside/Outside labels?</th> <th>Commentary</th> </tr> </thead> <tbody> <tr> <td><strong>ExtraSensory (UC San Diego, 2017)</strong></td> <td>Acc, Gyr, Mag, Air pressure, <strong>Light</strong>, <strong>22 kHz audio</strong>, phone-state, GPS, temp/humidity</td> <td>60 users ⇒ 302 k 20 s windows (≈22 h raw audio + high-freq motion; ~8 GB raw, ~215 MB features) (<a href="https://dcase-repo.github.io/dcase_datalist/datasets/scenes/extrasensory.html" title="DCASE Datalist / ExtraSensory Dataset">dcase-repo.github.io</a>, <a href="https://www.kaggle.com/datasets/yvaizman/the-extrasensory-dataset?utm_source=chatgpt.com" title="The ExtraSensory Dataset - Kaggle">kaggle.com</a>, <a href="https://dcase-repo.github.io/dcase_datalist/datasets/scenes/extrasensory.html?utm_source=chatgpt.com" title="DCASE Datalist / ExtraSensory Dataset">dcase-repo.github.io</a>)</td> <td>Yes (self-reported “indoors/outdoors” &amp; “in vehicle”) (<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC10915014/?utm_source=chatgpt.com" title="Robust human locomotion and localization activity recognition over ...">pmc.ncbi.nlm.nih.gov</a>)</td> <td><em>Full multimodal fusion</em> + noisy labels + class imbalance</td> </tr> <tr> <td><strong>Sussex-Huawei Locomotion (SHL, 2018)</strong></td> <td>Acc, Gyr, Mag, <strong>Microphone audio</strong>, Baro, GPS, humidity, 15 total phone sensors (<a href="https://www.researchgate.net/figure/Sensor-modalities-sampling-rate-in-the-complete-SHL-dataset_tbl1_354740102?utm_source=chatgpt.com" title="Sensor modalities (sampling rate) in the complete SHL dataset.">researchgate.net</a>, <a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices">eecs.qmul.ac.uk</a>)</td> <td>2 812 h; 950 GB raw; 4 phones × 59 days preview (~20 GB) (<a href="https://www.shl-dataset.org/dataset/?utm_source=chatgpt.com" title="Sussex-Huawei Locomotion Dataset">shl-dataset.org</a>, <a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices">eecs.qmul.ac.uk</a>)</td> <td>Yes (explicit inside/outside tag) (<a href="https://www.eecs.qmul.ac.uk/~linwang/download/papers/The%20university%20of%20sussex-huawei%20locomotion%20and%20transportation%20dataset%20for%20multimodal%20analytics%20with%20mobile%20devices.pdf" title="The University of Sussex-Huawei Locomotion and Transportation Dataset for Multimodal Analytics With Mobile Devices">eecs.qmul.ac.uk</a>)</td> <td><em>Scale &amp; placement robustness</em>; show distributed training</td> </tr> <tr> <td><strong>OutFin (Scientific Data, 2021)</strong></td> <td>Acc, Gyr, Mag, <strong>Baro</strong>, <strong>Light</strong> + Wi-Fi, BT, cellular (<a href="https://www.nature.com/articles/s41597-021-00832-y" title="OutFin, a multi-device and multi-modal dataset for outdoor localization based on the fingerprinting approach | Scientific Data">nature.com</a>, <a href="https://www.nature.com/articles/s41597-021-00832-y?utm_source=chatgpt.com" title="OutFin, a multi-device and multi-modal dataset for outdoor ... - Nature">nature.com</a>)</td> <td>122 outdoor sites, dense fingerprints (~5 GB) (<a href="https://www.nature.com/articles/s41597-021-00832-y" title="OutFin, a multi-device and multi-modal dataset for outdoor localization based on the fingerprinting approach | Scientific Data">nature.com</a>)</td> <td>Outdoor-only (good negative set)</td> <td><em>RF + sensor fusion</em> for GNSS-denied environments</td> </tr> <tr> <td><strong>Opportunity++ (2021)</strong></td> <td>72 sensors: body IMUs, object IMUs, ambient switches; video; <em>no light, no audio</em> (<a href="https://www.opportunity-project.eu/showcase.html?utm_source=chatgpt.com" title="Showcase | Opportunity">opportunity-project.eu</a>, <a href="https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2021.792065/full?utm_source=chatgpt.com" title="Opportunity++: A Multimodal Dataset for Video- and Wearable ...">frontiersin.org</a>)</td> <td>10 modalities, &gt;4 GB raw time-series/video</td> <td>Primarily indoor</td> <td><em>Wearable + ambient devices</em>; discuss aligning heterogeneous rates</td> </tr> </tbody> </table> <h1 id="preparing-the-data-for-a-model">Preparing the data for a model</h1> <p>As you can probably imagine, most time-series datasets are not entirely the cleanest. They’re also so information dense that its hard to apply the entire dataset for a model. This section, therefore, talks about the Lets start with the basics of the <code class="language-plaintext highlighter-rouge">ExtraSensory</code> dataset.</p> <ol> <li>It is a dataset from 60 users.</li> <li>Participants were asked to install a companion app on their phone.</li> <li>Time sampling was every 20 seconds for every minute. Data is boxed by the minute on the dataset.</li> <li>Participants were given an easy method to report the state that they were in (walking, driving, indoors, outdoors, etc.)</li> </ol> <p>Therefore, by definition the dataset is a multi-variate time-series dataset. Thinking about <a href="http://www.doorstep.ai">doorstep.ai</a>’s value proposition of providing precise dropoff location accuracy, I decided to focus on whether or not one could identify whether or not the delivery driver was outdoors or indoors. Ostensibly, this is a binary classification problem.</p> <h2 id="downloading-the-dataset">Downloading the dataset</h2> <p>A full guide to how I download the dataset is in the repository <a href="https://github.com/ajay-bhargava/extrasensory/blob/main/src/extrasensory/download/provision.py">here</a>. I make extensive use of <code class="language-plaintext highlighter-rouge">modal</code> because I love their serverless GPU platform oh so much. I will gush over them throughout this blog post so if you’re a <code class="language-plaintext highlighter-rouge">modal</code> hater then you can stop reading now.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CREDENTIALS</span> <span class="o">=</span> <span class="n">modal</span><span class="p">.</span><span class="n">Secret</span><span class="p">.</span><span class="nf">from_name</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">aws-secret</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">environment_name</span><span class="o">=</span><span class="sh">"</span><span class="s">main</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">required_keys</span><span class="o">=</span><span class="p">[</span>
        <span class="sh">"</span><span class="s">AWS_ACCESS_KEY_ID</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">AWS_SECRET_ACCESS_KEY</span><span class="sh">"</span><span class="p">,</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">modal</span><span class="p">.</span><span class="nc">App</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-download</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">image</span><span class="o">=</span><span class="n">download_image</span><span class="p">,</span>
<span class="p">)</span>

<span class="nd">@app.function</span><span class="p">(</span>
    <span class="n">volumes</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">/mnt</span><span class="sh">"</span><span class="p">:</span> <span class="n">modal</span><span class="p">.</span><span class="nc">CloudBucketMount</span><span class="p">(</span>
            <span class="n">bucket_name</span><span class="o">=</span><span class="n">BUCKET_NAME</span><span class="p">,</span>
            <span class="n">secret</span><span class="o">=</span><span class="n">CREDENTIALS</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">},</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">download_extrasensory_data</span><span class="p">():</span>
    <span class="bp">...</span>
</code></pre></div></div> <p>Out of principle when I’m working with machine learning datasets I try not to have anything downloaded to my computer. This is because of potential data leakage and security concerns on top of the fact that these datasets are freaking massive and as such are not a good candidate to just hold in my computer at all times. Fortunately with <code class="language-plaintext highlighter-rouge">modal</code> I can just mount any old S3 bucket as a path and then use it as a local path. This is pretty convenient.</p> <h2 id="discovering-temporal-windows">Discovering temporal windows</h2> <h3 id="intuition">Intuition</h3> <p>This dataset is a semi-continuous log of the phone’s sensors in multiple different contexts. If you were to just chuck all the data into a model, you’d likely have poor discrimination between the <code class="language-plaintext highlighter-rouge">outdoor</code> and <code class="language-plaintext highlighter-rouge">indoor</code> class because there are so many additional contexts that the person is in. For example a person can be outside but also inside a car. Indeed this is captured by the author and can be better described in the <a href="https://www.youtube.com/watch?v=2cuhvEQZ_sI&amp;themeRefresh=1">Youtube</a> video linked to this dataset. Furthermore, this graph clearly articulates all the captured contexts for a typical user in the study.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/extrasensory-data-scope-480.webp 480w,/assets/img/extrasensory-data-scope-800.webp 800w,/assets/img/extrasensory-data-scope-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/extrasensory-data-scope.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Diving a bit deeper, the dataset contains temporal windows of the following phone sensors:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <div class="caption">Accelerometer</div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/example_phone_acc-480.webp 480w,/assets/img/example_phone_acc-800.webp 800w,/assets/img/example_phone_acc-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/example_phone_acc.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <div class="caption">Audio</div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/example_audio-480.webp 480w,/assets/img/example_audio-800.webp 800w,/assets/img/example_audio-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/example_audio.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I had a thought. What if i narrow the context of the data to be used in the model to the point in the dataset where the person is transitioning between the outside and inside (e.g. immediately upon entering an enclosed building or leaving it?) This would increase the relevant number of samples to a value twice the number of users in the dataset.</p> \[N_{\text{after}} = 2\sum_{u=1}^{U} S_u = 2\,N_{\text{before}}\] <p>The number of samples after the temporal window is narrowed is twice the number of samples before the temporal window is narrowed. Implementing this in python required a bit of work but ultimately resulted in the desired effect, I had more temporal windows to work with that narrowly described the transition between the outside and inside class within a specified window of <code class="language-plaintext highlighter-rouge">window_size</code>.</p> <h3 id="implementation">Implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mark_transition_windows</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">col_inside</span><span class="p">,</span> <span class="n">col_outside</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">label_names</span><span class="p">,</span> <span class="n">new_label_names</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Mark transition windows in the data matrix X for transitions between two mutually exclusive labels.

    This function adds three new boolean columns to X:
        - The first new column is True for samples in a window before each transition.
        - The second new column is True for samples in a window after each transition.
        - The third new column is True at the exact transition point.
    The names for these new columns are provided by new_label_names and appended to label_names.

    Parameters
    ----------
    X : np.ndarray
        The input data matrix of shape (n_samples, n_features).
    col_inside : int
        The column index in X corresponding to the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> label (binary: 0 or 1).
    col_outside : int
        The column index in X corresponding to the </span><span class="sh">"</span><span class="s">outside</span><span class="sh">"</span><span class="s"> label (binary: 0 or 1).
    window_size : int
        The number of samples before and after the transition to mark as the window.
    label_names : list of str
        The list of existing label/feature names (length should match X.shape[1]).
    new_label_names : list of str
        The list of 3 names for the new columns (e.g., [</span><span class="sh">"</span><span class="s">left_window</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">right_window</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">transition</span><span class="sh">"</span><span class="s">]).

    Returns
    -------
    X_new : np.ndarray
        The augmented data matrix with three additional boolean columns indicating:
            - Before transition window
            - After transition window
            - At transition point
    label_names_new : list of str
        The updated list of label/feature names with the new column names appended.

    Notes
    -----
    - Assumes that the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> and </span><span class="sh">"</span><span class="s">outside</span><span class="sh">"</span><span class="s"> columns are mutually exclusive (never both 1).
    - Transitions are detected as changes in the </span><span class="sh">"</span><span class="s">inside</span><span class="sh">"</span><span class="s"> label.
    - If a transition window would extend beyond the bounds of X, it is skipped.

    Example
    -------
</span><span class="gp">    &gt;&gt;&gt;</span> <span class="n">X_aug</span><span class="p">,</span> <span class="n">label_names_aug</span> <span class="o">=</span> <span class="nf">mark_transition_windows</span><span class="p">(</span>
    <span class="p">...</span>     <span class="n">X</span><span class="p">,</span> <span class="n">col_inside</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col_outside</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="p">...</span>     <span class="n">label_names</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="p">...</span>     <span class="n">new_label_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">left_window</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">right_window</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">transition</span><span class="sh">"</span><span class="p">]</span>
    <span class="p">...</span> <span class="p">)</span>
    <span class="sh">"""</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">old_cols</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)])</span>
    <span class="n">inside</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">col_inside</span><span class="p">]</span>
    <span class="n">outside</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">col_outside</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">np</span><span class="p">.</span><span class="nf">all</span><span class="p">((</span><span class="n">inside</span> <span class="o">+</span> <span class="n">outside</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">),</span> <span class="sh">"</span><span class="s">Labels must be mutually exclusive</span><span class="sh">"</span>
    <span class="n">inside_diff</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diff</span><span class="p">(</span><span class="n">inside</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">),</span> <span class="n">prepend</span><span class="o">=</span><span class="n">inside</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">transitions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">inside_diff</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">transitions</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">-</span> <span class="n">window_size</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">idx</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">end</span> <span class="o">&gt;</span> <span class="n">n</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">X_new</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">idx</span><span class="p">,</span> <span class="n">old_cols</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>         <span class="c1"># Before
</span>        <span class="n">X_new</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="n">old_cols</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>       <span class="c1"># After
</span>        <span class="n">X_new</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="n">old_cols</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>             <span class="c1"># At transition
</span>    <span class="n">label_names_new</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">label_names</span><span class="p">)</span> <span class="o">+</span> <span class="nf">list</span><span class="p">(</span><span class="n">new_label_names</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_new</span><span class="p">,</span> <span class="n">label_names_new</span>
</code></pre></div></div> <h3 id="visual-description">Visual Description</h3> <p>Visually this could also be described as the graph below. The transition points are marked in purple with the before and after window of ~5 minutes marked in red and green respectively. The analysis isn’t completely perfect but it is directionally correct for the length of time I worked on this for a proof of concept.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-480.webp 480w,/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-800.webp 800w,/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/5EF64122-B513-46AE-BCF1-E62AAC285D2C.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="cleaning-and-preparing-the-dataset">Cleaning and preparing the dataset</h2> <p>I then ran a clean of the potential <code class="language-plaintext highlighter-rouge">np.nan</code> and <code class="language-plaintext highlighter-rouge">np.inf</code> values that would most defintiely interfere with a multi-variate classification model. This can be found in the github repository <a href="https://github.com/ajay-bhargava/extrasensory/blob/f9998b04509ba6cf2c96f66bc415044f2096ee96/src/extrasensory/prepare/utils/massage.py#L126-L189">here</a>. Running the code below resulted in a labels array that described the classes of the data. The final shape of the labels array was <code class="language-plaintext highlighter-rouge">(4293,)</code>. The final shape of the dataset was <code class="language-plaintext highlighter-rouge">(4293, 5, 225)</code></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Inside is 0, Outside is 1
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">inside_windows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">),</span> 
    <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">outside_windows</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)]</span>
<span class="p">)</span> 
</code></pre></div></div> <h1 id="building-the-model">Building the model</h1> <p>Its my belief that unless you’re writing a machine learning model from scratch (which you’re probably not going to be doing if you’re supporting most business cases) you’re going to be using a library to build your model. This has caveats. The caveats being that you’re beholden to the library’s design choices and support requirements to deploy your model. That being said, there are some pretty robust model libraries out there, and <code class="language-plaintext highlighter-rouge">tsai</code> is no exception. Training with <code class="language-plaintext highlighter-rouge">tsai</code> is easy because it is built ontop of <code class="language-plaintext highlighter-rouge">fastai</code>. Because of this, and recent developements to the <code class="language-plaintext highlighter-rouge">TSClassifier</code> class, deploying a model is as simple as running the code below. I will note there are a few caveats and these will be discussed.</p> <h2 id="the-tsai-library">The <code class="language-plaintext highlighter-rouge">tsai</code> library</h2> <p>Configuring a <code class="language-plaintext highlighter-rouge">TSClassifier</code> requires a dictionary with some typical parameters in any machine learning job:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Configruation Dictionary
</span><span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">tfms</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="n">TSClassification</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">batch_tfms</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">TSStandardize</span><span class="p">(</span><span class="n">by_sample</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span>
    <span class="sh">"</span><span class="s">metrics</span><span class="sh">"</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">arch</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">InceptionTimePlus</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">bs</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="mi">64</span><span class="p">,</span><span class="mi">128</span><span class="p">],</span>
    <span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">epochs</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> <p>I chose to split my data into test and train at the point of the model training. I prefer to keep my data as tight as possible to the model training process.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)),</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Splits
</span><span class="n">splits</span> <span class="o">=</span> <span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">train_index</span><span class="p">),</span> <span class="nf">list</span><span class="p">(</span><span class="n">val_index</span><span class="p">))</span>
</code></pre></div></div> <p>Then finally training the library is as simple as running the code below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create Model
</span><span class="n">learn</span> <span class="o">=</span> <span class="nc">TSClassifier</span><span class="p">(</span>
    <span class="n">X</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> 
    <span class="n">y</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> 
    <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> 
    <span class="n">tfms</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">tfms</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">batch_tfms</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">batch_tfms</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">metrics</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">metrics</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">arch</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">arch</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">bs</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">bs</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="nc">WandbCallback</span><span class="p">(</span><span class="n">log_preds</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">log_model</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-train</span><span class="sh">"</span><span class="p">)],</span>
<span class="p">)</span>

<span class="c1"># Train Model
</span><span class="n">learn</span><span class="p">.</span><span class="nf">fit_one_cycle</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">epochs</span><span class="sh">"</span><span class="p">],</span> <span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">lr</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div> <h2 id="inceptiontimeplus-intuition"><code class="language-plaintext highlighter-rouge">InceptionTimePlus</code> Intuition</h2> <p>I was at UofT doing my Masters in Biochemistry around the same time that Ilya Sutskever was working on the original AlexNet. There is a lot of influence from the original AlexNet in my own understanding of NN architectures and my background and interest in computer vision partly stems from this. I was therefore excited to use <code class="language-plaintext highlighter-rouge">InceptionTimePlus</code> as a model architecture for this project. Indeed the <a href="https://arxiv.org/pdf/1909.04939">paper</a> does pay quite a bit of homage to the original AlexNet when drawing parallels between the two model architectures.</p> <p>There are many ways to classify time-series data. Most of the approaches are interested in finding features (shapelets, spare-representations, etc.,) that are invariant to the time-index of the time-series.</p> <ol> <li> <p>Nearest Neighbor + NN-DTW: The NN-DTW approach is well described in this <a href="https://medium.com/walmartglobaltech/time-series-similarity-using-dynamic-time-warping-explained-9d09119e48ec">blog</a> post.</p> </li> <li> <p>BOSS: Breaks the time-series into symbolic features via discrete fourier transform. Then discretizes the features into bins, and if I’m getting this right, measures the euclidean distance between the features.</p> </li> </ol> <p><code class="language-plaintext highlighter-rouge">InceptionTime</code> works in a similar way as <code class="language-plaintext highlighter-rouge">AlexNet</code>. You have a sliding window of <code class="language-plaintext highlighter-rouge">m</code> filters on the multivariate time-series. In the case of my dataset its 225 features over 5 minutes. What this does is that it reduces the dimensionality of the time-series from 225 features to the number of filters. This “bottlenecking” reduces the multi-variate features of the feature space (that’s accelerometer, gyroscope etc.,) to a set of <code class="language-plaintext highlighter-rouge">m</code> filters.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/InceptionTime-Architecture-480.webp 480w,/assets/img/InceptionTime-Architecture-800.webp 800w,/assets/img/InceptionTime-Architecture-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/InceptionTime-Architecture.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>You can think of this the same way as a multi-dimensional image. If you’ve ever worked in science and have used the <code class="language-plaintext highlighter-rouge">.czi</code> format from Zeiss you know that a typical fluorescence image can be a 4-30 dimensional image based on the number of channels collected at various filter-band passes or excitations based on your laser. Time series works the same way in the multi-variate space. The time dimension is only collapsed in the very last layer. What happens there is just some pooling of the time-series against the collapsed <code class="language-plaintext highlighter-rouge">m</code> features from the sliding window over the time-series. This is also why making sure you choose a temporal window that’s cleanly defined is so important.</p> <p>There are obviously different are more interesting filters one could conjure up for time-series data but this method is a good starting point and a good model architecture.</p> <h2 id="weights-and-biases-callbacks">Weights and Biases Callbacks</h2> <p>You’re probably wondering what is this <code class="language-plaintext highlighter-rouge">WandbCallback</code>? Its a <code class="language-plaintext highlighter-rouge">fastai</code> callback that logs the training and validation metrics to <a href="https://wandb.ai/site">Weights and Biases</a>. I’ve found this to be a very useful tool for tracking the training of my models.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create Model
</span><span class="k">with</span> <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-train</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-train</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">learn</span> <span class="o">=</span> <span class="nc">TSClassifier</span><span class="p">(</span>
        <span class="n">cbs</span><span class="o">=</span><span class="p">[</span><span class="nc">WandbCallback</span><span class="p">(</span><span class="n">log_preds</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">log_model</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-train</span><span class="sh">"</span><span class="p">)],</span>
    <span class="p">)</span>
</code></pre></div></div> <p>Tracking training runs and validation metrics is then possible on <a href="https://wandb.ai/bhargava-ajay/extrasensory-train?nw=nwuserbhargavaajay">wandb.ai</a> which you can find here.</p> <h1 id="running-inference-on-using-the-model">Running inference on using the model</h1> <p>Running inference (owing to the amazing work of the <code class="language-plaintext highlighter-rouge">tsai</code> library) is as simple as running the code below.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@app.cls</span><span class="p">(</span>
    <span class="n">image</span><span class="o">=</span><span class="n">inference_image</span><span class="p">,</span> 
    <span class="n">gpu</span><span class="o">=</span><span class="sh">"</span><span class="s">any</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">scaledown_window</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>
    <span class="n">volumes</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">/weights</span><span class="sh">"</span><span class="p">:</span> <span class="n">weights_volume</span><span class="p">},</span>
<span class="p">)</span>
<span class="k">class</span> <span class="nc">Inference</span><span class="p">:</span>
    <span class="nd">@modal.enter</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">disabled</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nf">load_learner</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">/weights/</span><span class="si">{</span><span class="n">MODEL_ID</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">cpu</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
    <span class="nd">@modal.method</span><span class="p">()</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nf">get_X_preds</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">with_input</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>You can call this from an ASGI remotely as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">####
# Request Model
####
</span>
<span class="k">class</span> <span class="nc">Request</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">array</span><span class="p">:</span> <span class="nb">str</span>
   
<span class="k">class</span> <span class="nc">Result</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">probabilities</span><span class="p">:</span> <span class="nb">list</span>
    <span class="n">predicted_class</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">message</span><span class="p">:</span> <span class="nb">str</span>
    
<span class="c1">####
# ASGI Application
####
</span>
<span class="nd">@app.function</span><span class="p">(</span>
    <span class="n">image</span><span class="o">=</span><span class="n">asgi_image</span><span class="p">,</span>
    <span class="n">max_containers</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
<span class="nd">@modal.asgi_app</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">extrasensory-inference</span><span class="sh">"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">extrasensory_inference</span><span class="p">():</span>
    <span class="n">application</span> <span class="o">=</span> <span class="nc">FastAPI</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Extrasensory Inference</span><span class="sh">"</span><span class="p">,</span> <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">Inference for the Extrasensory Model</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># Inference endpoint
</span>    <span class="nd">@application.post</span><span class="p">(</span><span class="sh">"</span><span class="s">/predict</span><span class="sh">"</span><span class="p">,</span> <span class="n">response_model</span><span class="o">=</span><span class="n">Result</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">Request</span> <span class="o">=</span> <span class="nc">Body</span><span class="p">()):</span>
        
        <span class="nb">bytes</span> <span class="o">=</span> <span class="n">base64</span><span class="p">.</span><span class="nf">b64decode</span><span class="p">(</span><span class="n">request</span><span class="p">.</span><span class="n">array</span><span class="p">)</span>
        <span class="n">array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">io</span><span class="p">.</span><span class="nc">BytesIO</span><span class="p">(</span><span class="nb">bytes</span><span class="p">))</span>
        
        <span class="k">if</span> <span class="n">array</span><span class="p">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">225</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">error</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Invalid array shape</span><span class="sh">"</span><span class="p">}</span>

        <span class="c1"># Make prediction - don't await the remote call
</span>        <span class="n">response</span> <span class="o">=</span> <span class="nc">Inference</span><span class="p">().</span><span class="n">predict</span><span class="p">.</span><span class="nf">remote</span><span class="p">(</span><span class="n">array</span><span class="p">)</span> <span class="c1"># type: ignore
</span>        
        <span class="c1"># Unpack the tuple and convert tensor to numpy
</span>        <span class="n">probs_tensor</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">class_label</span> <span class="o">=</span> <span class="n">response</span>
        <span class="n">probabilities</span> <span class="o">=</span> <span class="n">probs_tensor</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>
        <span class="n">predicted_class</span> <span class="o">=</span> <span class="n">class_label</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="nc">Result</span><span class="p">(</span>
            <span class="n">probabilities</span><span class="o">=</span><span class="n">probabilities</span><span class="p">,</span>
            <span class="n">predicted_class</span><span class="o">=</span><span class="n">predicted_class</span><span class="p">,</span>
            <span class="n">message</span><span class="o">=</span><span class="sh">"</span><span class="s">Success</span><span class="sh">"</span>
        <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">application</span>
</code></pre></div></div> <p>It will return a JSON object with the probabilities of the predicted class and the predicted class label.</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
    </span><span class="nl">"probabilities"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">0.9999999403953552</span><span class="p">,</span><span class="w"> </span><span class="mf">4.964479923248291e-07</span><span class="p">],</span><span class="w">
    </span><span class="nl">"predicted_class"</span><span class="p">:</span><span class="w"> </span><span class="s2">"outside"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Success"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <h1 id="caveats">Caveats</h1> <h2 id="wandb-callbacks"><code class="language-plaintext highlighter-rouge">wandb Callbacks</code></h2> <p>One very peculiar thing I noticed and spent a needless amount of time on was the need to have the <code class="language-plaintext highlighter-rouge">wandb</code> callback or at least the <code class="language-plaintext highlighter-rouge">wandb</code> library instantiated before the prediction model was instantiated. I thought it was weird that the pickled model required the <code class="language-plaintext highlighter-rouge">wandb</code> library. This might’ve been due to the fact that one of the model callbacks was a <code class="language-plaintext highlighter-rouge">wandb</code> callback.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="sh">"</span><span class="s">disabled</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="sparsity-of-the-dataset">Sparsity of the dataset</h2> <p>The <code class="language-plaintext highlighter-rouge">Extrasensory</code> dataset was sampled at a rate of 1 minute. Events such as walking between inside and outside happen at a much faster frequency than 1 minute. This means that the model and its ability to generalize for indoor versus outdoor classification might be limited. Ideally you’d want a sampling frequency of at least once per second or even faster. This would pose challenges from a battery life perspective and potentially pose challenges if the data was uploaded to the cloud when the phone is in wifi range.</p> <h1 id="gushing-over-modal">Gushing over Modal</h1> <p>Its no secret that I gush over Modal. I particularly liked the ability to use <code class="language-plaintext highlighter-rouge">modal.CloudBucketMount</code> to mount a S3 bucket as a local path and then a <code class="language-plaintext highlighter-rouge">modal.Volume</code> to mount a volume to the container. Container definition on Modal is just so dead simple. Defining infrastructure should be this easy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@app.function</span><span class="p">(</span>
    <span class="n">volumes</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">/mnt/</span><span class="sh">"</span><span class="p">:</span> <span class="n">modal</span><span class="p">.</span><span class="nc">CloudBucketMount</span><span class="p">(</span>
            <span class="n">bucket_name</span><span class="o">=</span><span class="n">BUCKET_NAME</span><span class="p">,</span>
            <span class="n">secret</span><span class="o">=</span><span class="n">CREDENTIALS</span>
        <span class="p">),</span>
        <span class="sh">"</span><span class="s">/weights</span><span class="sh">"</span><span class="p">:</span> <span class="n">weights</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">secrets</span><span class="o">=</span><span class="p">[</span><span class="n">WANDB_SECRET</span><span class="p">],</span>
    <span class="n">gpu</span><span class="o">=</span><span class="sh">"</span><span class="s">any</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div> <p>Some of my next learnings are going to be how to chain all these files together into a single pipeline. It would be cool to just run one script to download the data, train the model, and deploy the model.</p> <h1 id="conclusion">Conclusion</h1> <p>Even though I didn’t get the job at Doorstep.ai, I am happy to have had the opportunity to work on this project. I really love working on time-series data and I’m excited for others to take note of the codebase for their own projects.</p>]]></content><author><name></name></author><category term="machine-learning-posts"/><category term="machine-learning"/><category term="timeseries"/><category term="9-DoF"/><category term="sensor"/><category term="modal"/><category term="python"/><category term="tsai"/><summary type="html"><![CDATA[Reflections on building a time-series classification model that uses the data from the 9-DoF sensor on your phone.]]></summary></entry><entry><title type="html">Great questions are the predicate to scalable execution</title><link href="https://ajay-bhargava.github.io/blog/2025/scalable-execution/" rel="alternate" type="text/html" title="Great questions are the predicate to scalable execution"/><published>2025-06-11T01:00:00+00:00</published><updated>2025-06-11T01:00:00+00:00</updated><id>https://ajay-bhargava.github.io/blog/2025/scalable-execution</id><content type="html" xml:base="https://ajay-bhargava.github.io/blog/2025/scalable-execution/"><![CDATA[<p>When I was a new bench scientist, I was often plagued by what I called the “hand of crap”: failed protein purifications, contaminated cell lines, botched experiments. It wasn’t just bad luck or inexperience. It was mindset. A mindset rooted in scarcity, distrust, and control.</p> <p>I hated handing off experiments. No one else could possibly “see” what I saw or do it “right.” I believed that if I didn’t do the science myself, it wasn’t real. And so, I spun my wheels, designing experiments that were fundamentally flawed because the question itself was bad. I was, frankly, full of shit.</p> <p>A few years later, after a humbling and productive stint at a biotech startup in NYC, I started my PhD at the Francis Crick Institute in London. It was there that everything shifted.</p> <p>I learned that the “hand of crap” isn’t a curse it’s a consequence of weak ideas. Strong hypotheses yield informative outcomes, regardless of which way the data goes. Great scientists design experiments where every outcome tells you something valuable. And more importantly, they choose problems that require minimal manipulation of nature.</p> <p>This was liberating.</p> <p>Suddenly, I wasn’t worried about breaking the system. The biology just was. Whether I’d just flown back from Paris or finished a gym session, the phenomena were robust and reproducible.</p> <blockquote> <p>I’m writing up a manuscript on how the rulebook of spatiotemporal tumor heterogeneity is underwritten by Newtonian physics.</p> </blockquote> <p>This confidence had a compounding effect. I started trusting the Crick’s core facilities—plasmid sequencing, FACS, histopathology, microscopy, even mouse husbandry. I wasn’t relying on other postdocs or grad students to scale my work. I was leveraging world-class infrastructure to move faster. My role was to think deeply, coordinate precisely, and execute smartly.</p> <p>As the projects grew, so did the quality of my collaborators. They weren’t generalists. They were niche technology experts—people who had built custom tools that could be integrated into my workflow. It was no accident. The Crick was built for this kind of velocity. Their Scientific Technology Platforms (STPs) were low on bureaucracy and high on readiness. Execution was just… fast.</p> <p>Having come from industry, I saw what many didn’t: the Crick was structured like a biotech company. The core facilities weren’t just helpful, they were essential to increasing DBTL (Design, Build, Test, Learn) cycles. Scientists who understood this system who knew how to orchestrate across this distributed network were winning. Fast.</p> <p>And if you don’t believe me, just look at the publication record.</p>]]></content><author><name></name></author><category term="science-posts"/><category term="science"/><category term="research"/><category term="mindset"/><category term="execution"/><category term="dbtl"/><summary type="html"><![CDATA[Reflections on mindset, infrastructure, and scientific velocity—from the Crick to startup biotech.]]></summary></entry></feed>